{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18591fc3-65bb-4539-bf51-d6477db34c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfa064cc-086d-4599-bd53-3daf0705769a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "houseprices = read_repo_data('lanre4PF', \"House-Price-Prediction-Ames-Housing-Dataset-\")\n",
    "alien_invasion = read_repo_data('lanre4PF', \"Alien-invasion-clone-\")\n",
    "\n",
    "print(len(houseprices))\n",
    "print(len(alien_invasion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74a94479-cafa-4548-bea1-ba4a6d9a5dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': '# üè† House Price Prediction (Kaggle - Ames Housing Dataset)\\n\\nA machine learning project for predicting house prices using the **Ames Housing Dataset**, as featured in the Kaggle competition *House Prices: Advanced Regression Techniques*. This project applies regression models to estimate house values based on property features such as location, size, quality, and condition.\\n\\n---\\n\\n## üìå Project Overview\\nThis repository demonstrates the application of machine learning algorithms to structured/tabular data prediction.  \\n- **Dataset**: [Kaggle ‚Äì House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) (2,930 properties, 80+ features).  \\n- **Frameworks**: **Scikit-learn, Pandas, NumPy, Matplotlib/Seaborn, XGBoost**  \\n- **Task**: Supervised learning (regression).  \\n\\nThe goal is to preprocess the dataset, build predictive models, and evaluate their performance in estimating house prices.\\n\\n---\\n\\n## üß† Features\\n- Data preprocessing (handling missing values, encoding categorical features, feature scaling).  \\n- Exploratory Data Analysis (EDA) with data visualization.  \\n- XGBoost\\n- Model evaluation using **RMSE, MAE, and R¬≤ Score**.  \\n- Hyperparameter tuning with GridSearchCV/.  \\n- Visualization of feature importance and residual errors.  \\n- Submission file generation for Kaggle competition.  \\n\\n---', 'filename': 'House-Price-Prediction-Ames-Housing-Dataset--main/README.md'}]\n"
     ]
    }
   ],
   "source": [
    "print(houseprices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04b65575-bc51-4df7-8ec2-08ce6704e917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': \"# üëæ Alien Invasion Clone  \\n\\nA fun **2D arcade-style game** built with Python‚Äôs `pygame` library. Inspired by the classic *Space Invaders*, this project is a clone of the **Alien Invasion** game from Eric Matthes' *Python Crash Course*.  \\n\\n---\\n\\n## üöÄ Features  \\n- Player-controlled spaceship with smooth movement.  \\n- Laser bullets to shoot down alien fleets.  \\n- Increasing difficulty with each wave.  \\n- Game-over and restart mechanics.  \\n- Simple yet engaging retro-style gameplay.  \\n\\n---\\n\\n## üõ†Ô∏è Tech Stack  \\n- **Python 3.x**  \\n- **Pygame**  \\n\\n---\\n\\n## üì¶ Installation  \\n\\nClone the repository and install dependencies:  \\n\\n```bash\\n# Clone the repo\\ngit clone https://github.com/your-username/alien-invasion-clone.git\\ncd alien-invasion-clone\\n\\n# (Optional) Create virtual environment\\npython -m venv venv\\nsource venv/bin/activate   # On Mac/Linux\\nvenv\\\\Scripts\\\\activate      # On Windows\", 'filename': 'Alien-invasion-clone--main/README.md'}]\n"
     ]
    }
   ],
   "source": [
    "print(alien_invasion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a056121-d7c8-48f2-9288-4424382a63b8",
   "metadata": {},
   "source": [
    "# Simple Chunking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d297ffa-c51b-4ca2-b7a8-aca216322d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics: 409\n"
     ]
    }
   ],
   "source": [
    "ultralytics = read_repo_data('ultralytics', 'ultralytics')\n",
    "print(f\"ultralytics: {len(ultralytics)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1615380-cc5c-4109-856d-a96f84bf9833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics-main/CONTRIBUTING.md\n",
      "ultralytics-main/README.md\n",
      "ultralytics-main/README.zh-CN.md\n",
      "ultralytics-main/docs/README.md\n",
      "ultralytics-main/docs/coming_soon_template.md\n",
      "ultralytics-main/docs/en/datasets/classify/caltech101.md\n",
      "ultralytics-main/docs/en/datasets/classify/caltech256.md\n",
      "ultralytics-main/docs/en/datasets/classify/cifar10.md\n",
      "ultralytics-main/docs/en/datasets/classify/cifar100.md\n",
      "ultralytics-main/docs/en/datasets/classify/fashion-mnist.md\n",
      "ultralytics-main/docs/en/datasets/classify/imagenet.md\n",
      "ultralytics-main/docs/en/datasets/classify/imagenet10.md\n",
      "ultralytics-main/docs/en/datasets/classify/imagenette.md\n",
      "ultralytics-main/docs/en/datasets/classify/imagewoof.md\n",
      "ultralytics-main/docs/en/datasets/classify/index.md\n",
      "ultralytics-main/docs/en/datasets/classify/mnist.md\n",
      "ultralytics-main/docs/en/datasets/detect/african-wildlife.md\n",
      "ultralytics-main/docs/en/datasets/detect/argoverse.md\n",
      "ultralytics-main/docs/en/datasets/detect/brain-tumor.md\n",
      "ultralytics-main/docs/en/datasets/detect/coco.md\n",
      "ultralytics-main/docs/en/datasets/detect/coco128.md\n",
      "ultralytics-main/docs/en/datasets/detect/coco8-grayscale.md\n",
      "ultralytics-main/docs/en/datasets/detect/coco8-multispectral.md\n",
      "ultralytics-main/docs/en/datasets/detect/coco8.md\n",
      "ultralytics-main/docs/en/datasets/detect/construction-ppe.md\n",
      "ultralytics-main/docs/en/datasets/detect/globalwheat2020.md\n",
      "ultralytics-main/docs/en/datasets/detect/homeobjects-3k.md\n",
      "ultralytics-main/docs/en/datasets/detect/index.md\n",
      "ultralytics-main/docs/en/datasets/detect/lvis.md\n",
      "ultralytics-main/docs/en/datasets/detect/medical-pills.md\n",
      "ultralytics-main/docs/en/datasets/detect/objects365.md\n",
      "ultralytics-main/docs/en/datasets/detect/open-images-v7.md\n",
      "ultralytics-main/docs/en/datasets/detect/roboflow-100.md\n",
      "ultralytics-main/docs/en/datasets/detect/signature.md\n",
      "ultralytics-main/docs/en/datasets/detect/sku-110k.md\n",
      "ultralytics-main/docs/en/datasets/detect/visdrone.md\n",
      "ultralytics-main/docs/en/datasets/detect/voc.md\n",
      "ultralytics-main/docs/en/datasets/detect/xview.md\n",
      "ultralytics-main/docs/en/datasets/explorer/api.md\n",
      "ultralytics-main/docs/en/datasets/explorer/dashboard.md\n",
      "ultralytics-main/docs/en/datasets/explorer/explorer.md\n",
      "ultralytics-main/docs/en/datasets/explorer/index.md\n",
      "ultralytics-main/docs/en/datasets/index.md\n",
      "ultralytics-main/docs/en/datasets/obb/dota-v2.md\n",
      "ultralytics-main/docs/en/datasets/obb/dota8.md\n",
      "ultralytics-main/docs/en/datasets/obb/index.md\n",
      "ultralytics-main/docs/en/datasets/pose/coco.md\n",
      "ultralytics-main/docs/en/datasets/pose/coco8-pose.md\n",
      "ultralytics-main/docs/en/datasets/pose/dog-pose.md\n",
      "ultralytics-main/docs/en/datasets/pose/hand-keypoints.md\n",
      "ultralytics-main/docs/en/datasets/pose/index.md\n",
      "ultralytics-main/docs/en/datasets/pose/tiger-pose.md\n",
      "ultralytics-main/docs/en/datasets/segment/carparts-seg.md\n",
      "ultralytics-main/docs/en/datasets/segment/coco.md\n",
      "ultralytics-main/docs/en/datasets/segment/coco128-seg.md\n",
      "ultralytics-main/docs/en/datasets/segment/coco8-seg.md\n",
      "ultralytics-main/docs/en/datasets/segment/crack-seg.md\n",
      "ultralytics-main/docs/en/datasets/segment/index.md\n",
      "ultralytics-main/docs/en/datasets/segment/package-seg.md\n",
      "ultralytics-main/docs/en/datasets/track/index.md\n",
      "ultralytics-main/docs/en/guides/analytics.md\n",
      "ultralytics-main/docs/en/guides/azureml-quickstart.md\n",
      "ultralytics-main/docs/en/guides/conda-quickstart.md\n",
      "ultralytics-main/docs/en/guides/coral-edge-tpu-on-raspberry-pi.md\n",
      "ultralytics-main/docs/en/guides/data-collection-and-annotation.md\n",
      "ultralytics-main/docs/en/guides/deepstream-nvidia-jetson.md\n",
      "ultralytics-main/docs/en/guides/defining-project-goals.md\n",
      "ultralytics-main/docs/en/guides/distance-calculation.md\n",
      "ultralytics-main/docs/en/guides/docker-quickstart.md\n",
      "ultralytics-main/docs/en/guides/heatmaps.md\n",
      "ultralytics-main/docs/en/guides/hyperparameter-tuning.md\n",
      "ultralytics-main/docs/en/guides/index.md\n",
      "ultralytics-main/docs/en/guides/instance-segmentation-and-tracking.md\n",
      "ultralytics-main/docs/en/guides/isolating-segmentation-objects.md\n",
      "ultralytics-main/docs/en/guides/kfold-cross-validation.md\n",
      "ultralytics-main/docs/en/guides/model-deployment-options.md\n",
      "ultralytics-main/docs/en/guides/model-deployment-practices.md\n",
      "ultralytics-main/docs/en/guides/model-evaluation-insights.md\n",
      "ultralytics-main/docs/en/guides/model-monitoring-and-maintenance.md\n",
      "ultralytics-main/docs/en/guides/model-testing.md\n",
      "ultralytics-main/docs/en/guides/model-training-tips.md\n",
      "ultralytics-main/docs/en/guides/model-yaml-config.md\n",
      "ultralytics-main/docs/en/guides/nvidia-jetson.md\n",
      "ultralytics-main/docs/en/guides/object-blurring.md\n",
      "ultralytics-main/docs/en/guides/object-counting.md\n",
      "ultralytics-main/docs/en/guides/object-cropping.md\n",
      "ultralytics-main/docs/en/guides/optimizing-openvino-latency-vs-throughput-modes.md\n",
      "ultralytics-main/docs/en/guides/parking-management.md\n",
      "ultralytics-main/docs/en/guides/preprocessing_annotated_data.md\n",
      "ultralytics-main/docs/en/guides/queue-management.md\n",
      "ultralytics-main/docs/en/guides/raspberry-pi.md\n",
      "ultralytics-main/docs/en/guides/region-counting.md\n",
      "ultralytics-main/docs/en/guides/ros-quickstart.md\n",
      "ultralytics-main/docs/en/guides/sahi-tiled-inference.md\n",
      "ultralytics-main/docs/en/guides/security-alarm-system.md\n",
      "ultralytics-main/docs/en/guides/similarity-search.md\n",
      "ultralytics-main/docs/en/guides/speed-estimation.md\n",
      "ultralytics-main/docs/en/guides/steps-of-a-cv-project.md\n",
      "ultralytics-main/docs/en/guides/streamlit-live-inference.md\n",
      "ultralytics-main/docs/en/guides/trackzone.md\n",
      "ultralytics-main/docs/en/guides/triton-inference-server.md\n",
      "ultralytics-main/docs/en/guides/vertex-ai-deployment-with-docker.md\n",
      "ultralytics-main/docs/en/guides/view-results-in-terminal.md\n",
      "ultralytics-main/docs/en/guides/vision-eye.md\n",
      "ultralytics-main/docs/en/guides/workouts-monitoring.md\n",
      "ultralytics-main/docs/en/guides/yolo-common-issues.md\n",
      "ultralytics-main/docs/en/guides/yolo-data-augmentation.md\n",
      "ultralytics-main/docs/en/guides/yolo-performance-metrics.md\n",
      "ultralytics-main/docs/en/guides/yolo-thread-safe-inference.md\n",
      "ultralytics-main/docs/en/help/CI.md\n",
      "ultralytics-main/docs/en/help/CLA.md\n",
      "ultralytics-main/docs/en/help/FAQ.md\n",
      "ultralytics-main/docs/en/help/code-of-conduct.md\n",
      "ultralytics-main/docs/en/help/contributing.md\n",
      "ultralytics-main/docs/en/help/environmental-health-safety.md\n",
      "ultralytics-main/docs/en/help/index.md\n",
      "ultralytics-main/docs/en/help/minimum-reproducible-example.md\n",
      "ultralytics-main/docs/en/help/privacy.md\n",
      "ultralytics-main/docs/en/help/security.md\n",
      "ultralytics-main/docs/en/hub/api/index.md\n",
      "ultralytics-main/docs/en/hub/app/android.md\n",
      "ultralytics-main/docs/en/hub/app/index.md\n",
      "ultralytics-main/docs/en/hub/app/ios.md\n",
      "ultralytics-main/docs/en/hub/cloud-training.md\n",
      "ultralytics-main/docs/en/hub/datasets.md\n",
      "ultralytics-main/docs/en/hub/index.md\n",
      "ultralytics-main/docs/en/hub/inference-api.md\n",
      "ultralytics-main/docs/en/hub/integrations.md\n",
      "ultralytics-main/docs/en/hub/models.md\n",
      "ultralytics-main/docs/en/hub/pro.md\n",
      "ultralytics-main/docs/en/hub/projects.md\n",
      "ultralytics-main/docs/en/hub/quickstart.md\n",
      "ultralytics-main/docs/en/hub/teams.md\n",
      "ultralytics-main/docs/en/index.md\n",
      "ultralytics-main/docs/en/integrations/albumentations.md\n",
      "ultralytics-main/docs/en/integrations/amazon-sagemaker.md\n",
      "ultralytics-main/docs/en/integrations/clearml.md\n",
      "ultralytics-main/docs/en/integrations/comet.md\n",
      "ultralytics-main/docs/en/integrations/coreml.md\n",
      "ultralytics-main/docs/en/integrations/dvc.md\n",
      "ultralytics-main/docs/en/integrations/edge-tpu.md\n",
      "ultralytics-main/docs/en/integrations/google-colab.md\n",
      "ultralytics-main/docs/en/integrations/gradio.md\n",
      "ultralytics-main/docs/en/integrations/ibm-watsonx.md\n",
      "ultralytics-main/docs/en/integrations/index.md\n",
      "ultralytics-main/docs/en/integrations/jupyterlab.md\n",
      "ultralytics-main/docs/en/integrations/kaggle.md\n",
      "ultralytics-main/docs/en/integrations/mlflow.md\n",
      "ultralytics-main/docs/en/integrations/mnn.md\n",
      "ultralytics-main/docs/en/integrations/ncnn.md\n",
      "ultralytics-main/docs/en/integrations/neural-magic.md\n",
      "ultralytics-main/docs/en/integrations/onnx.md\n",
      "ultralytics-main/docs/en/integrations/openvino.md\n",
      "ultralytics-main/docs/en/integrations/paddlepaddle.md\n",
      "ultralytics-main/docs/en/integrations/paperspace.md\n",
      "ultralytics-main/docs/en/integrations/ray-tune.md\n",
      "ultralytics-main/docs/en/integrations/roboflow.md\n",
      "ultralytics-main/docs/en/integrations/rockchip-rknn.md\n",
      "ultralytics-main/docs/en/integrations/seeedstudio-recamera.md\n",
      "ultralytics-main/docs/en/integrations/sony-imx500.md\n",
      "ultralytics-main/docs/en/integrations/tensorboard.md\n",
      "ultralytics-main/docs/en/integrations/tensorrt.md\n",
      "ultralytics-main/docs/en/integrations/tf-graphdef.md\n",
      "ultralytics-main/docs/en/integrations/tf-savedmodel.md\n",
      "ultralytics-main/docs/en/integrations/tfjs.md\n",
      "ultralytics-main/docs/en/integrations/tflite.md\n",
      "ultralytics-main/docs/en/integrations/torchscript.md\n",
      "ultralytics-main/docs/en/integrations/vscode.md\n",
      "ultralytics-main/docs/en/integrations/weights-biases.md\n",
      "ultralytics-main/docs/en/macros/augmentation-args.md\n",
      "ultralytics-main/docs/en/macros/export-args.md\n",
      "ultralytics-main/docs/en/macros/export-table.md\n",
      "ultralytics-main/docs/en/macros/predict-args.md\n",
      "ultralytics-main/docs/en/macros/sam-auto-annotate.md\n",
      "ultralytics-main/docs/en/macros/solutions-args.md\n",
      "ultralytics-main/docs/en/macros/track-args.md\n",
      "ultralytics-main/docs/en/macros/train-args.md\n",
      "ultralytics-main/docs/en/macros/validation-args.md\n",
      "ultralytics-main/docs/en/macros/visualization-args.md\n",
      "ultralytics-main/docs/en/macros/yolo-cls-perf.md\n",
      "ultralytics-main/docs/en/macros/yolo-det-perf.md\n",
      "ultralytics-main/docs/en/macros/yolo-obb-perf.md\n",
      "ultralytics-main/docs/en/macros/yolo-pose-perf.md\n",
      "ultralytics-main/docs/en/macros/yolo-seg-perf.md\n",
      "ultralytics-main/docs/en/models/fast-sam.md\n",
      "ultralytics-main/docs/en/models/index.md\n",
      "ultralytics-main/docs/en/models/mobile-sam.md\n",
      "ultralytics-main/docs/en/models/rtdetr.md\n",
      "ultralytics-main/docs/en/models/sam-2.md\n",
      "ultralytics-main/docs/en/models/sam.md\n",
      "ultralytics-main/docs/en/models/yolo-nas.md\n",
      "ultralytics-main/docs/en/models/yolo-world.md\n",
      "ultralytics-main/docs/en/models/yolo11.md\n",
      "ultralytics-main/docs/en/models/yolo12.md\n",
      "ultralytics-main/docs/en/models/yolo26.md\n",
      "ultralytics-main/docs/en/models/yoloe.md\n",
      "ultralytics-main/docs/en/models/yolov10.md\n",
      "ultralytics-main/docs/en/models/yolov3.md\n",
      "ultralytics-main/docs/en/models/yolov4.md\n",
      "ultralytics-main/docs/en/models/yolov5.md\n",
      "ultralytics-main/docs/en/models/yolov6.md\n",
      "ultralytics-main/docs/en/models/yolov7.md\n",
      "ultralytics-main/docs/en/models/yolov8.md\n",
      "ultralytics-main/docs/en/models/yolov9.md\n",
      "ultralytics-main/docs/en/modes/benchmark.md\n",
      "ultralytics-main/docs/en/modes/export.md\n",
      "ultralytics-main/docs/en/modes/index.md\n",
      "ultralytics-main/docs/en/modes/predict.md\n",
      "ultralytics-main/docs/en/modes/track.md\n",
      "ultralytics-main/docs/en/modes/train.md\n",
      "ultralytics-main/docs/en/modes/val.md\n",
      "ultralytics-main/docs/en/quickstart.md\n",
      "ultralytics-main/docs/en/reference/__init__.md\n",
      "ultralytics-main/docs/en/reference/cfg/__init__.md\n",
      "ultralytics-main/docs/en/reference/data/annotator.md\n",
      "ultralytics-main/docs/en/reference/data/augment.md\n",
      "ultralytics-main/docs/en/reference/data/base.md\n",
      "ultralytics-main/docs/en/reference/data/build.md\n",
      "ultralytics-main/docs/en/reference/data/converter.md\n",
      "ultralytics-main/docs/en/reference/data/dataset.md\n",
      "ultralytics-main/docs/en/reference/data/loaders.md\n",
      "ultralytics-main/docs/en/reference/data/split.md\n",
      "ultralytics-main/docs/en/reference/data/split_dota.md\n",
      "ultralytics-main/docs/en/reference/data/utils.md\n",
      "ultralytics-main/docs/en/reference/engine/exporter.md\n",
      "ultralytics-main/docs/en/reference/engine/model.md\n",
      "ultralytics-main/docs/en/reference/engine/predictor.md\n",
      "ultralytics-main/docs/en/reference/engine/results.md\n",
      "ultralytics-main/docs/en/reference/engine/trainer.md\n",
      "ultralytics-main/docs/en/reference/engine/tuner.md\n",
      "ultralytics-main/docs/en/reference/engine/validator.md\n",
      "ultralytics-main/docs/en/reference/hub/__init__.md\n",
      "ultralytics-main/docs/en/reference/hub/auth.md\n",
      "ultralytics-main/docs/en/reference/hub/google/__init__.md\n",
      "ultralytics-main/docs/en/reference/hub/session.md\n",
      "ultralytics-main/docs/en/reference/hub/utils.md\n",
      "ultralytics-main/docs/en/reference/models/fastsam/model.md\n",
      "ultralytics-main/docs/en/reference/models/fastsam/predict.md\n",
      "ultralytics-main/docs/en/reference/models/fastsam/utils.md\n",
      "ultralytics-main/docs/en/reference/models/fastsam/val.md\n",
      "ultralytics-main/docs/en/reference/models/nas/model.md\n",
      "ultralytics-main/docs/en/reference/models/nas/predict.md\n",
      "ultralytics-main/docs/en/reference/models/nas/val.md\n",
      "ultralytics-main/docs/en/reference/models/rtdetr/model.md\n",
      "ultralytics-main/docs/en/reference/models/rtdetr/predict.md\n",
      "ultralytics-main/docs/en/reference/models/rtdetr/train.md\n",
      "ultralytics-main/docs/en/reference/models/rtdetr/val.md\n",
      "ultralytics-main/docs/en/reference/models/sam/amg.md\n",
      "ultralytics-main/docs/en/reference/models/sam/build.md\n",
      "ultralytics-main/docs/en/reference/models/sam/model.md\n",
      "ultralytics-main/docs/en/reference/models/sam/modules/blocks.md\n",
      "ultralytics-main/docs/en/reference/models/sam/modules/decoders.md\n",
      "ultralytics-main/docs/en/reference/models/sam/modules/encoders.md\n",
      "ultralytics-main/docs/en/reference/models/sam/modules/memory_attention.md\n",
      "ultralytics-main/docs/en/reference/models/sam/modules/sam.md\n",
      "ultralytics-main/docs/en/reference/models/sam/modules/tiny_encoder.md\n",
      "ultralytics-main/docs/en/reference/models/sam/modules/transformer.md\n",
      "ultralytics-main/docs/en/reference/models/sam/modules/utils.md\n",
      "ultralytics-main/docs/en/reference/models/sam/predict.md\n",
      "ultralytics-main/docs/en/reference/models/utils/loss.md\n",
      "ultralytics-main/docs/en/reference/models/utils/ops.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/classify/predict.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/classify/train.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/classify/val.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/detect/predict.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/detect/train.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/detect/val.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/model.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/obb/predict.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/obb/train.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/obb/val.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/pose/predict.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/pose/train.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/pose/val.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/segment/predict.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/segment/train.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/segment/val.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/world/train.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/world/train_world.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/yoloe/predict.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/yoloe/train.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/yoloe/train_seg.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/yoloe/val.md\n",
      "ultralytics-main/docs/en/reference/nn/autobackend.md\n",
      "ultralytics-main/docs/en/reference/nn/modules/activation.md\n",
      "ultralytics-main/docs/en/reference/nn/modules/block.md\n",
      "ultralytics-main/docs/en/reference/nn/modules/conv.md\n",
      "ultralytics-main/docs/en/reference/nn/modules/head.md\n",
      "ultralytics-main/docs/en/reference/nn/modules/transformer.md\n",
      "ultralytics-main/docs/en/reference/nn/modules/utils.md\n",
      "ultralytics-main/docs/en/reference/nn/tasks.md\n",
      "ultralytics-main/docs/en/reference/nn/text_model.md\n",
      "ultralytics-main/docs/en/reference/solutions/ai_gym.md\n",
      "ultralytics-main/docs/en/reference/solutions/analytics.md\n",
      "ultralytics-main/docs/en/reference/solutions/config.md\n",
      "ultralytics-main/docs/en/reference/solutions/distance_calculation.md\n",
      "ultralytics-main/docs/en/reference/solutions/heatmap.md\n",
      "ultralytics-main/docs/en/reference/solutions/instance_segmentation.md\n",
      "ultralytics-main/docs/en/reference/solutions/object_blurrer.md\n",
      "ultralytics-main/docs/en/reference/solutions/object_counter.md\n",
      "ultralytics-main/docs/en/reference/solutions/object_cropper.md\n",
      "ultralytics-main/docs/en/reference/solutions/parking_management.md\n",
      "ultralytics-main/docs/en/reference/solutions/queue_management.md\n",
      "ultralytics-main/docs/en/reference/solutions/region_counter.md\n",
      "ultralytics-main/docs/en/reference/solutions/security_alarm.md\n",
      "ultralytics-main/docs/en/reference/solutions/similarity_search.md\n",
      "ultralytics-main/docs/en/reference/solutions/solutions.md\n",
      "ultralytics-main/docs/en/reference/solutions/speed_estimation.md\n",
      "ultralytics-main/docs/en/reference/solutions/streamlit_inference.md\n",
      "ultralytics-main/docs/en/reference/solutions/trackzone.md\n",
      "ultralytics-main/docs/en/reference/solutions/vision_eye.md\n",
      "ultralytics-main/docs/en/reference/trackers/basetrack.md\n",
      "ultralytics-main/docs/en/reference/trackers/bot_sort.md\n",
      "ultralytics-main/docs/en/reference/trackers/byte_tracker.md\n",
      "ultralytics-main/docs/en/reference/trackers/track.md\n",
      "ultralytics-main/docs/en/reference/trackers/utils/gmc.md\n",
      "ultralytics-main/docs/en/reference/trackers/utils/kalman_filter.md\n",
      "ultralytics-main/docs/en/reference/trackers/utils/matching.md\n",
      "ultralytics-main/docs/en/reference/utils/__init__.md\n",
      "ultralytics-main/docs/en/reference/utils/autobatch.md\n",
      "ultralytics-main/docs/en/reference/utils/autodevice.md\n",
      "ultralytics-main/docs/en/reference/utils/benchmarks.md\n",
      "ultralytics-main/docs/en/reference/utils/callbacks/base.md\n",
      "ultralytics-main/docs/en/reference/utils/callbacks/clearml.md\n",
      "ultralytics-main/docs/en/reference/utils/callbacks/comet.md\n",
      "ultralytics-main/docs/en/reference/utils/callbacks/dvc.md\n",
      "ultralytics-main/docs/en/reference/utils/callbacks/hub.md\n",
      "ultralytics-main/docs/en/reference/utils/callbacks/mlflow.md\n",
      "ultralytics-main/docs/en/reference/utils/callbacks/neptune.md\n",
      "ultralytics-main/docs/en/reference/utils/callbacks/platform.md\n",
      "ultralytics-main/docs/en/reference/utils/callbacks/raytune.md\n",
      "ultralytics-main/docs/en/reference/utils/callbacks/tensorboard.md\n",
      "ultralytics-main/docs/en/reference/utils/callbacks/wb.md\n",
      "ultralytics-main/docs/en/reference/utils/checks.md\n",
      "ultralytics-main/docs/en/reference/utils/cpu.md\n",
      "ultralytics-main/docs/en/reference/utils/dist.md\n",
      "ultralytics-main/docs/en/reference/utils/downloads.md\n",
      "ultralytics-main/docs/en/reference/utils/errors.md\n",
      "ultralytics-main/docs/en/reference/utils/events.md\n",
      "ultralytics-main/docs/en/reference/utils/export/__init__.md\n",
      "ultralytics-main/docs/en/reference/utils/export/imx.md\n",
      "ultralytics-main/docs/en/reference/utils/files.md\n",
      "ultralytics-main/docs/en/reference/utils/git.md\n",
      "ultralytics-main/docs/en/reference/utils/instance.md\n",
      "ultralytics-main/docs/en/reference/utils/logger.md\n",
      "ultralytics-main/docs/en/reference/utils/loss.md\n",
      "ultralytics-main/docs/en/reference/utils/metrics.md\n",
      "ultralytics-main/docs/en/reference/utils/nms.md\n",
      "ultralytics-main/docs/en/reference/utils/ops.md\n",
      "ultralytics-main/docs/en/reference/utils/patches.md\n",
      "ultralytics-main/docs/en/reference/utils/plotting.md\n",
      "ultralytics-main/docs/en/reference/utils/tal.md\n",
      "ultralytics-main/docs/en/reference/utils/torch_utils.md\n",
      "ultralytics-main/docs/en/reference/utils/tqdm.md\n",
      "ultralytics-main/docs/en/reference/utils/triton.md\n",
      "ultralytics-main/docs/en/reference/utils/tuner.md\n",
      "ultralytics-main/docs/en/solutions/index.md\n",
      "ultralytics-main/docs/en/tasks/classify.md\n",
      "ultralytics-main/docs/en/tasks/detect.md\n",
      "ultralytics-main/docs/en/tasks/index.md\n",
      "ultralytics-main/docs/en/tasks/obb.md\n",
      "ultralytics-main/docs/en/tasks/pose.md\n",
      "ultralytics-main/docs/en/tasks/segment.md\n",
      "ultralytics-main/docs/en/usage/callbacks.md\n",
      "ultralytics-main/docs/en/usage/cfg.md\n",
      "ultralytics-main/docs/en/usage/cli.md\n",
      "ultralytics-main/docs/en/usage/engine.md\n",
      "ultralytics-main/docs/en/usage/python.md\n",
      "ultralytics-main/docs/en/usage/simple-utilities.md\n",
      "ultralytics-main/docs/en/yolov5/environments/aws_quickstart_tutorial.md\n",
      "ultralytics-main/docs/en/yolov5/environments/azureml_quickstart_tutorial.md\n",
      "ultralytics-main/docs/en/yolov5/environments/docker_image_quickstart_tutorial.md\n",
      "ultralytics-main/docs/en/yolov5/environments/google_cloud_quickstart_tutorial.md\n",
      "ultralytics-main/docs/en/yolov5/index.md\n",
      "ultralytics-main/docs/en/yolov5/quickstart_tutorial.md\n",
      "ultralytics-main/docs/en/yolov5/tutorials/architecture_description.md\n",
      "ultralytics-main/docs/en/yolov5/tutorials/clearml_logging_integration.md\n",
      "ultralytics-main/docs/en/yolov5/tutorials/comet_logging_integration.md\n",
      "ultralytics-main/docs/en/yolov5/tutorials/hyperparameter_evolution.md\n",
      "ultralytics-main/docs/en/yolov5/tutorials/model_ensembling.md\n",
      "ultralytics-main/docs/en/yolov5/tutorials/model_export.md\n",
      "ultralytics-main/docs/en/yolov5/tutorials/model_pruning_and_sparsity.md\n",
      "ultralytics-main/docs/en/yolov5/tutorials/multi_gpu_training.md\n",
      "ultralytics-main/docs/en/yolov5/tutorials/neural_magic_pruning_quantization.md\n",
      "ultralytics-main/docs/en/yolov5/tutorials/pytorch_hub_model_loading.md\n",
      "ultralytics-main/docs/en/yolov5/tutorials/test_time_augmentation.md\n",
      "ultralytics-main/docs/en/yolov5/tutorials/tips_for_best_training_results.md\n",
      "ultralytics-main/docs/en/yolov5/tutorials/train_custom_data.md\n",
      "ultralytics-main/docs/en/yolov5/tutorials/transfer_learning_with_frozen_layers.md\n",
      "ultralytics-main/examples/README.md\n",
      "ultralytics-main/examples/RTDETR-ONNXRuntime-Python/README.md\n",
      "ultralytics-main/examples/YOLO-Interactive-Tracking-UI/README.md\n",
      "ultralytics-main/examples/YOLO-Series-ONNXRuntime-Rust/README.md\n",
      "ultralytics-main/examples/YOLO11-Triton-CPP/README.md\n",
      "ultralytics-main/examples/YOLOv8-Action-Recognition/README.md\n",
      "ultralytics-main/examples/YOLOv8-CPP-Inference/README.md\n",
      "ultralytics-main/examples/YOLOv8-LibTorch-CPP-Inference/README.md\n",
      "ultralytics-main/examples/YOLOv8-MNN-CPP/README.md\n",
      "ultralytics-main/examples/YOLOv8-ONNXRuntime-CPP/README.md\n",
      "ultralytics-main/examples/YOLOv8-ONNXRuntime-Rust/README.md\n",
      "ultralytics-main/examples/YOLOv8-ONNXRuntime/README.md\n",
      "ultralytics-main/examples/YOLOv8-OpenCV-ONNX-Python/README.md\n",
      "ultralytics-main/examples/YOLOv8-OpenVINO-CPP-Inference/README.md\n",
      "ultralytics-main/examples/YOLOv8-Region-Counter/README.md\n",
      "ultralytics-main/examples/YOLOv8-SAHI-Inference-Video/README.md\n",
      "ultralytics-main/examples/YOLOv8-Segmentation-ONNXRuntime-Python/README.md\n",
      "ultralytics-main/examples/YOLOv8-TFLite-Python/README.md\n",
      "ultralytics-main/ultralytics/cfg/models/README.md\n",
      "ultralytics-main/ultralytics/trackers/README.md\n"
     ]
    }
   ],
   "source": [
    "for data in ultralytics: \n",
    "    print (data[\"filename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab7cea59-39c9-450c-bd55-3aaf9be94368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3314179-1c41-46b9-a143-7a4dddf3c3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ultralytics_chunks =[]\n",
    "\n",
    "for doc in ultralytics:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop(\"content\")\n",
    "    chunks = sliding_window(doc_content, 2000, 1000 )\n",
    "    for chunk in chunks: \n",
    "        chunk.update(doc_copy)\n",
    "    ultralytics_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "377e0c38-a278-40b4-8c77-8fcc2e846b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3382\n"
     ]
    }
   ],
   "source": [
    "print(len(ultralytics_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb18126-d0f0-4b35-bf7a-1baca5483624",
   "metadata": {},
   "source": [
    "we obtained 3382 chunks from 409 documents "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98f1e39-0b83-4383-b3bc-ecb8beb5eb37",
   "metadata": {},
   "source": [
    "# Paragraph chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c61b023e-79da-4dc4-8aaa-3652cc1d2bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = ultralytics[45]['content']\n",
    "paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f03b369-bd8b-48e5-84fa-ac78009a9318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Oriented Bounding Box (OBB) Datasets Overview',\n",
       " 'Training a precise [object detection](https://www.ultralytics.com/glossary/object-detection) model with oriented bounding boxes (OBB) requires a thorough dataset. This guide explains the various OBB dataset formats compatible with Ultralytics YOLO models, offering insights into their structure, application, and methods for format conversions.',\n",
       " '## Supported OBB Dataset Formats',\n",
       " '### YOLO OBB Format',\n",
       " 'The YOLO OBB format designates bounding boxes by their four corner points with coordinates normalized between 0 and 1. It follows this format:',\n",
       " '```bash\\nclass_index x1 y1 x2 y2 x3 y3 x4 y4\\n```',\n",
       " \"Internally, YOLO processes losses and outputs in the `xywhr` format, which represents the [bounding box](https://www.ultralytics.com/glossary/bounding-box)'s center point (xy), width, height, and rotation.\",\n",
       " '<p align=\"center\"><img width=\"800\" src=\"https://github.com/ultralytics/docs/releases/download/0/obb-format-examples.avif\" alt=\"OBB format examples\"></p>',\n",
       " 'An example of a `*.txt` label file for the above image, which contains an object of class `0` in OBB format, could look like:',\n",
       " '```bash\\n0 0.780811 0.743961 0.782371 0.74686 0.777691 0.752174 0.776131 0.749758\\n```',\n",
       " '### Dataset YAML format',\n",
       " 'The Ultralytics framework uses a YAML file format to define the dataset and model configuration for training OBB Models. Here is an example of the YAML format used for defining a OBB dataset:',\n",
       " '!!! example \"ultralytics/cfg/datasets/dota8.yaml\"',\n",
       " '    ```yaml\\n    --8<-- \"ultralytics/cfg/datasets/dota8.yaml\"\\n    ```',\n",
       " '## Usage',\n",
       " 'To train a model using these OBB formats:',\n",
       " '!!! example',\n",
       " '    === \"Python\"',\n",
       " '        ```python\\n        from ultralytics import YOLO',\n",
       " '        # Create a new YOLO11n-OBB model from scratch\\n        model = YOLO(\"yolo11n-obb.yaml\")',\n",
       " '        # Train the model on the DOTAv1 dataset\\n        results = model.train(data=\"DOTAv1.yaml\", epochs=100, imgsz=1024)\\n        ```',\n",
       " '    === \"CLI\"',\n",
       " '        ```bash\\n        # Train a new YOLO11n-OBB model on the DOTAv1 dataset\\n        yolo obb train data=DOTAv1.yaml model=yolo11n-obb.pt epochs=100 imgsz=1024\\n        ```',\n",
       " '## Supported Datasets',\n",
       " 'Currently, the following datasets with Oriented Bounding Boxes are supported:',\n",
       " '- [DOTA-v1](dota-v2.md): The first version of the DOTA dataset, providing a comprehensive set of aerial images with oriented bounding boxes for object detection.\\n- [DOTA-v1.5](dota-v2.md): An intermediate version of the DOTA dataset, offering additional annotations and improvements over DOTA-v1 for enhanced object detection tasks.\\n- [DOTA-v2](dota-v2.md): DOTA (A Large-scale Dataset for Object Detection in Aerial Images) version 2, emphasizes detection from aerial perspectives and contains oriented bounding boxes with 1.7 million instances and 11,268 images.\\n- [DOTA8](dota8.md): A small, 8-image subset of the full DOTA dataset suitable for testing workflows and Continuous Integration (CI) checks of OBB training in the `ultralytics` repository.',\n",
       " '### Incorporating your own OBB dataset',\n",
       " 'For those looking to introduce their own datasets with oriented bounding boxes, ensure compatibility with the \"YOLO OBB format\" mentioned above. Convert your annotations to this required format and detail the paths, classes, and class names in a corresponding YAML configuration file.',\n",
       " '## Convert Label Formats',\n",
       " '### DOTA Dataset Format to YOLO OBB Format',\n",
       " 'Transitioning labels from the DOTA dataset format to the YOLO OBB format can be achieved with this script:',\n",
       " '!!! example',\n",
       " '    === \"Python\"',\n",
       " '        ```python\\n        from ultralytics.data.converter import convert_dota_to_yolo_obb',\n",
       " '        convert_dota_to_yolo_obb(\"path/to/DOTA\")\\n        ```',\n",
       " 'This conversion mechanism is instrumental for datasets in the DOTA format, ensuring alignment with the [Ultralytics YOLO](../../models/yolo11.md) OBB format.',\n",
       " \"It's imperative to validate the compatibility of the dataset with your model and adhere to the necessary format conventions. Properly structured datasets are pivotal for training efficient object detection models with oriented bounding boxes.\",\n",
       " '## FAQ',\n",
       " '### What are Oriented Bounding Boxes (OBB) and how are they used in Ultralytics YOLO models?',\n",
       " 'Oriented Bounding Boxes (OBB) are a type of bounding box annotation where the box can be rotated to align more closely with the object being detected, rather than just being axis-aligned. This is particularly useful in aerial or satellite imagery where objects might not be aligned with the image axes. In [Ultralytics YOLO](../../tasks/obb.md) models, OBBs are represented by their four corner points in the YOLO OBB format. This allows for more accurate object detection since the bounding boxes can rotate to fit the objects better.',\n",
       " '### How do I convert my existing DOTA dataset labels to YOLO OBB format for use with Ultralytics YOLO11?',\n",
       " \"You can convert DOTA dataset labels to YOLO OBB format using the [`convert_dota_to_yolo_obb`](../../reference/data/converter.md) function from Ultralytics. This conversion ensures compatibility with the Ultralytics YOLO models, enabling you to leverage the OBB capabilities for enhanced object detection. Here's a quick example:\",\n",
       " '```python\\nfrom ultralytics.data.converter import convert_dota_to_yolo_obb',\n",
       " 'convert_dota_to_yolo_obb(\"path/to/DOTA\")\\n```',\n",
       " 'This script will reformat your DOTA annotations into a YOLO-compatible format.',\n",
       " '### How do I train a YOLO11 model with oriented bounding boxes (OBB) on my dataset?',\n",
       " \"Training a YOLO11 model with OBBs involves ensuring your dataset is in the YOLO OBB format and then using the [Ultralytics API](../../usage/python.md) to train the model. Here's an example in both Python and CLI:\",\n",
       " '!!! example',\n",
       " '    === \"Python\"',\n",
       " '        ```python\\n        from ultralytics import YOLO',\n",
       " '        # Create a new YOLO11n-OBB model from scratch\\n        model = YOLO(\"yolo11n-obb.yaml\")',\n",
       " '        # Train the model on the custom dataset\\n        results = model.train(data=\"your_dataset.yaml\", epochs=100, imgsz=640)\\n        ```',\n",
       " '    === \"CLI\"',\n",
       " '        ```bash\\n        # Train a new YOLO11n-OBB model on the custom dataset\\n        yolo obb train data=your_dataset.yaml model=yolo11n-obb.yaml epochs=100 imgsz=640\\n        ```',\n",
       " 'This ensures your model leverages the detailed OBB annotations for improved detection [accuracy](https://www.ultralytics.com/glossary/accuracy).',\n",
       " '### What datasets are currently supported for OBB training in Ultralytics YOLO models?',\n",
       " 'Currently, Ultralytics supports the following datasets for OBB training:',\n",
       " '- [DOTA-v1](dota-v2.md): The first version of the DOTA dataset, providing a comprehensive set of aerial images with oriented bounding boxes for object detection.\\n- [DOTA-v1.5](dota-v2.md): An intermediate version of the DOTA dataset, offering additional annotations and improvements over DOTA-v1 for enhanced object detection tasks.\\n- [DOTA-v2](dota-v2.md): This dataset includes 1.7 million instances with oriented bounding boxes and 11,268 images, primarily focusing on aerial object detection.\\n- [DOTA8](dota8.md): A smaller, 8-image subset of the DOTA dataset used for testing and [continuous integration](../../help/CI.md) (CI) checks.',\n",
       " 'These datasets are tailored for scenarios where OBBs offer a significant advantage, such as aerial and satellite image analysis.',\n",
       " '### Can I use my own dataset with oriented bounding boxes for YOLO11 training, and if so, how?',\n",
       " 'Yes, you can use your own dataset with oriented bounding boxes for YOLO11 training. Ensure your dataset annotations are converted to the YOLO OBB format, which involves defining bounding boxes by their four corner points. You can then create a [YAML configuration file](../../usage/cfg.md) specifying the dataset paths, classes, and other necessary details. For more information on creating and configuring your datasets, refer to the [Supported Datasets](#supported-datasets) section.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389994e6-2bc1-4756-b26b-d14844f791ff",
   "metadata": {},
   "source": [
    "# chunking by sections and headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a52234c-3d7f-4c9c-b10e-30fe4c89b193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_markdown_by_level(text, level=2):\n",
    "    \"\"\"\n",
    "    Split markdown text by a specific header level.\n",
    "    \n",
    "    :param text: Markdown text as a string\n",
    "    :param level: Header level to split on\n",
    "    :return: List of sections as strings\n",
    "    \"\"\"\n",
    "    # This regex matches markdown headers\n",
    "    # For level 2, it matches lines starting with \"## \"\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "    # Split and keep the headers\n",
    "    parts = pattern.split(text)\n",
    "    \n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        # We step by 3 because regex.split() with\n",
    "        # capturing groups returns:\n",
    "        # [before_match, group1, group2, after_match, ...]\n",
    "        # here group1 is \"## \", group2 is the header text\n",
    "        header = parts[i] + parts[i+1]  # \"## \" + \"Title\"\n",
    "        header = header.strip()\n",
    "\n",
    "        # Get the content after this header\n",
    "        content = \"\"\n",
    "        if i+2 < len(parts):\n",
    "            content = parts[i+2].strip()\n",
    "\n",
    "        if content:\n",
    "            section = f'{header}\\n\\n{content}'\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "    \n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fe462e5-1483-46e0-9b6d-63294e3e0f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ultralytics_chunks2 = []\n",
    "\n",
    "for doc in ultralytics:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        ultralytics_chunks2.append(section_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f3818bc-1e7a-4157-9f1e-3cdf10bdcaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2459\n"
     ]
    }
   ],
   "source": [
    "print(len(ultralytics_chunks2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553677d9-0665-4c6b-96da-0e96f6faf47b",
   "metadata": {},
   "source": [
    "# Intelligent chunking "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a12fe1-08ae-4df2-b7b6-790be3a6850f",
   "metadata": {},
   "source": [
    "This process requires time and incurs costs. As mentioned before, use this only when really necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25f54027-845c-44da-aba1-0e18318b970e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bc9202-7883-4b73-b873-8f89d1271c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "\n",
    "def llm(prompt, model='gpt-4o-mini'):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model='gpt-4o-mini',\n",
    "        input=messages\n",
    "    )\n",
    "\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5702322d-4ebe-48b3-8123-ec2f5f0b15d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec85d918-52f6-4a60-96bf-08985976e689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intelligent_chunking(text):\n",
    "    prompt = prompt_template.format(document=text)\n",
    "    response = llm(prompt)\n",
    "    sections = response.split('---')\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6dae44-dce7-4c7b-813f-a2b00ff48321",
   "metadata": {},
   "outputs": [],
   "source": [
    "ultralytics_chunks3 = []\n",
    "for doc in tqdm(ultralytics_chunks3):\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        ultralytics_chunks3.append(section_doc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
