{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8fe904c-3b0d-4a04-ad8e-a0313425084a",
   "metadata": {},
   "source": [
    "# Day 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18591fc3-65bb-4539-bf51-d6477db34c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfa064cc-086d-4599-bd53-3daf0705769a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "houseprices = read_repo_data('lanre4PF', \"House-Price-Prediction-Ames-Housing-Dataset-\")\n",
    "alien_invasion = read_repo_data('lanre4PF', \"Alien-invasion-clone-\")\n",
    "\n",
    "print(len(houseprices))\n",
    "print(len(alien_invasion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74a94479-cafa-4548-bea1-ba4a6d9a5dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': '# üè† House Price Prediction (Kaggle - Ames Housing Dataset)\\n\\nA machine learning project for predicting house prices using the **Ames Housing Dataset**, as featured in the Kaggle competition *House Prices: Advanced Regression Techniques*. This project applies regression models to estimate house values based on property features such as location, size, quality, and condition.\\n\\n---\\n\\n## üìå Project Overview\\nThis repository demonstrates the application of machine learning algorithms to structured/tabular data prediction.  \\n- **Dataset**: [Kaggle ‚Äì House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) (2,930 properties, 80+ features).  \\n- **Frameworks**: **Scikit-learn, Pandas, NumPy, Matplotlib/Seaborn, XGBoost**  \\n- **Task**: Supervised learning (regression).  \\n\\nThe goal is to preprocess the dataset, build predictive models, and evaluate their performance in estimating house prices.\\n\\n---\\n\\n## üß† Features\\n- Data preprocessing (handling missing values, encoding categorical features, feature scaling).  \\n- Exploratory Data Analysis (EDA) with data visualization.  \\n- XGBoost\\n- Model evaluation using **RMSE, MAE, and R¬≤ Score**.  \\n- Hyperparameter tuning with GridSearchCV/.  \\n- Visualization of feature importance and residual errors.  \\n- Submission file generation for Kaggle competition.  \\n\\n---', 'filename': 'House-Price-Prediction-Ames-Housing-Dataset--main/README.md'}]\n"
     ]
    }
   ],
   "source": [
    "print(houseprices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04b65575-bc51-4df7-8ec2-08ce6704e917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': \"# üëæ Alien Invasion Clone  \\n\\nA fun **2D arcade-style game** built with Python‚Äôs `pygame` library. Inspired by the classic *Space Invaders*, this project is a clone of the **Alien Invasion** game from Eric Matthes' *Python Crash Course*.  \\n\\n---\\n\\n## üöÄ Features  \\n- Player-controlled spaceship with smooth movement.  \\n- Laser bullets to shoot down alien fleets.  \\n- Increasing difficulty with each wave.  \\n- Game-over and restart mechanics.  \\n- Simple yet engaging retro-style gameplay.  \\n\\n---\\n\\n## üõ†Ô∏è Tech Stack  \\n- **Python 3.x**  \\n- **Pygame**  \\n\\n---\\n\\n## üì¶ Installation  \\n\\nClone the repository and install dependencies:  \\n\\n```bash\\n# Clone the repo\\ngit clone https://github.com/your-username/alien-invasion-clone.git\\ncd alien-invasion-clone\\n\\n# (Optional) Create virtual environment\\npython -m venv venv\\nsource venv/bin/activate   # On Mac/Linux\\nvenv\\\\Scripts\\\\activate      # On Windows\", 'filename': 'Alien-invasion-clone--main/README.md'}]\n"
     ]
    }
   ],
   "source": [
    "print(alien_invasion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7294a88-4801-470e-9b58-0ee257cf4a57",
   "metadata": {},
   "source": [
    "# Day 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a056121-d7c8-48f2-9288-4424382a63b8",
   "metadata": {},
   "source": [
    "# Simple Chunking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d297ffa-c51b-4ca2-b7a8-aca216322d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics: 409\n"
     ]
    }
   ],
   "source": [
    "ultralytics = read_repo_data('ultralytics', 'ultralytics')\n",
    "print(f\"ultralytics: {len(ultralytics)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1615380-cc5c-4109-856d-a96f84bf9833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics-main/CONTRIBUTING.md\n",
      "ultralytics-main/README.md\n",
      "ultralytics-main/README.zh-CN.md\n",
      "ultralytics-main/docs/README.md\n",
      "ultralytics-main/docs/coming_soon_template.md\n",
      "ultralytics-main/docs/en/datasets/classify/caltech101.md\n",
      "ultralytics-main/docs/en/datasets/classify/caltech256.md\n",
      "ultralytics-main/docs/en/datasets/classify/cifar10.md\n",
      "ultralytics-main/docs/en/datasets/classify/cifar100.md\n",
      "ultralytics-main/docs/en/datasets/classify/fashion-mnist.md\n",
      "ultralytics-main/docs/en/datasets/classify/imagenet.md\n",
      "ultralytics-main/docs/en/datasets/classify/imagenet10.md\n",
      "ultralytics-main/docs/en/datasets/classify/imagenette.md\n",
      "ultralytics-main/docs/en/datasets/classify/imagewoof.md\n",
      "ultralytics-main/docs/en/datasets/classify/index.md\n",
      "ultralytics-main/docs/en/datasets/classify/mnist.md\n",
      "ultralytics-main/docs/en/datasets/detect/african-wildlife.md\n",
      "ultralytics-main/docs/en/datasets/detect/argoverse.md\n",
      "ultralytics-main/docs/en/datasets/detect/brain-tumor.md\n",
      "ultralytics-main/docs/en/datasets/detect/coco.md\n",
      "ultralytics-main/docs/en/datasets/detect/coco128.md\n",
      "ultralytics-main/docs/en/datasets/detect/coco8-grayscale.md\n",
      "ultralytics-main/docs/en/datasets/detect/coco8-multispectral.md\n",
      "ultralytics-main/docs/en/datasets/detect/coco8.md\n",
      "ultralytics-main/docs/en/datasets/detect/construction-ppe.md\n",
      "ultralytics-main/docs/en/datasets/detect/globalwheat2020.md\n",
      "ultralytics-main/docs/en/datasets/detect/homeobjects-3k.md\n",
      "ultralytics-main/docs/en/datasets/detect/index.md\n",
      "ultralytics-main/docs/en/datasets/detect/lvis.md\n",
      "ultralytics-main/docs/en/datasets/detect/medical-pills.md\n",
      "ultralytics-main/docs/en/datasets/detect/objects365.md\n",
      "ultralytics-main/docs/en/datasets/detect/open-images-v7.md\n",
      "ultralytics-main/docs/en/datasets/detect/roboflow-100.md\n",
      "ultralytics-main/docs/en/datasets/detect/signature.md\n",
      "ultralytics-main/docs/en/datasets/detect/sku-110k.md\n",
      "ultralytics-main/docs/en/datasets/detect/visdrone.md\n",
      "ultralytics-main/docs/en/datasets/detect/voc.md\n",
      "ultralytics-main/docs/en/datasets/detect/xview.md\n",
      "ultralytics-main/docs/en/datasets/explorer/api.md\n",
      "ultralytics-main/docs/en/datasets/explorer/dashboard.md\n",
      "ultralytics-main/docs/en/datasets/explorer/explorer.md\n",
      "ultralytics-main/docs/en/datasets/explorer/index.md\n",
      "ultralytics-main/docs/en/datasets/index.md\n",
      "ultralytics-main/docs/en/datasets/obb/dota-v2.md\n",
      "ultralytics-main/docs/en/datasets/obb/dota8.md\n",
      "ultralytics-main/docs/en/datasets/obb/index.md\n",
      "ultralytics-main/docs/en/datasets/pose/coco.md\n",
      "ultralytics-main/docs/en/datasets/pose/coco8-pose.md\n",
      "ultralytics-main/docs/en/datasets/pose/dog-pose.md\n",
      "ultralytics-main/docs/en/datasets/pose/hand-keypoints.md\n",
      "ultralytics-main/docs/en/datasets/pose/index.md\n",
      "ultralytics-main/docs/en/datasets/pose/tiger-pose.md\n",
      "ultralytics-main/docs/en/datasets/segment/carparts-seg.md\n",
      "ultralytics-main/docs/en/datasets/segment/coco.md\n",
      "ultralytics-main/docs/en/datasets/segment/coco128-seg.md\n",
      "ultralytics-main/docs/en/datasets/segment/coco8-seg.md\n",
      "ultralytics-main/docs/en/datasets/segment/crack-seg.md\n",
      "ultralytics-main/docs/en/datasets/segment/index.md\n",
      "ultralytics-main/docs/en/datasets/segment/package-seg.md\n",
      "ultralytics-main/docs/en/datasets/track/index.md\n",
      "ultralytics-main/docs/en/guides/analytics.md\n",
      "ultralytics-main/docs/en/guides/azureml-quickstart.md\n",
      "ultralytics-main/docs/en/guides/conda-quickstart.md\n",
      "ultralytics-main/docs/en/guides/coral-edge-tpu-on-raspberry-pi.md\n",
      "ultralytics-main/docs/en/guides/data-collection-and-annotation.md\n",
      "ultralytics-main/docs/en/guides/deepstream-nvidia-jetson.md\n",
      "ultralytics-main/docs/en/guides/defining-project-goals.md\n",
      "ultralytics-main/docs/en/guides/distance-calculation.md\n",
      "ultralytics-main/docs/en/guides/docker-quickstart.md\n",
      "ultralytics-main/docs/en/guides/heatmaps.md\n",
      "ultralytics-main/docs/en/guides/hyperparameter-tuning.md\n",
      "ultralytics-main/docs/en/guides/index.md\n",
      "ultralytics-main/docs/en/guides/instance-segmentation-and-tracking.md\n",
      "ultralytics-main/docs/en/guides/isolating-segmentation-objects.md\n",
      "ultralytics-main/docs/en/guides/kfold-cross-validation.md\n",
      "ultralytics-main/docs/en/guides/model-deployment-options.md\n",
      "ultralytics-main/docs/en/guides/model-deployment-practices.md\n",
      "ultralytics-main/docs/en/guides/model-evaluation-insights.md\n",
      "ultralytics-main/docs/en/guides/model-monitoring-and-maintenance.md\n",
      "ultralytics-main/docs/en/guides/model-testing.md\n",
      "ultralytics-main/docs/en/guides/model-training-tips.md\n",
      "ultralytics-main/docs/en/guides/model-yaml-config.md\n",
      "ultralytics-main/docs/en/guides/nvidia-jetson.md\n",
      "ultralytics-main/docs/en/guides/object-blurring.md\n",
      "ultralytics-main/docs/en/guides/object-counting.md\n",
      "ultralytics-main/docs/en/guides/object-cropping.md\n",
      "ultralytics-main/docs/en/guides/optimizing-openvino-latency-vs-throughput-modes.md\n",
      "ultralytics-main/docs/en/guides/parking-management.md\n",
      "ultralytics-main/docs/en/guides/preprocessing_annotated_data.md\n",
      "ultralytics-main/docs/en/guides/queue-management.md\n",
      "ultralytics-main/docs/en/guides/raspberry-pi.md\n",
      "ultralytics-main/docs/en/guides/region-counting.md\n",
      "ultralytics-main/docs/en/guides/ros-quickstart.md\n",
      "ultralytics-main/docs/en/guides/sahi-tiled-inference.md\n",
      "ultralytics-main/docs/en/guides/security-alarm-system.md\n",
      "ultralytics-main/docs/en/guides/similarity-search.md\n",
      "ultralytics-main/docs/en/guides/speed-estimation.md\n",
      "ultralytics-main/docs/en/guides/steps-of-a-cv-project.md\n",
      "ultralytics-main/docs/en/guides/streamlit-live-inference.md\n",
      "ultralytics-main/docs/en/guides/trackzone.md\n",
      "ultralytics-main/docs/en/guides/triton-inference-server.md\n",
      "ultralytics-main/docs/en/guides/vertex-ai-deployment-with-docker.md\n",
      "ultralytics-main/docs/en/guides/view-results-in-terminal.md\n",
      "ultralytics-main/docs/en/guides/vision-eye.md\n",
      "ultralytics-main/docs/en/guides/workouts-monitoring.md\n",
      "ultralytics-main/docs/en/guides/yolo-common-issues.md\n",
      "ultralytics-main/docs/en/guides/yolo-data-augmentation.md\n",
      "ultralytics-main/docs/en/guides/yolo-performance-metrics.md\n",
      "ultralytics-main/docs/en/guides/yolo-thread-safe-inference.md\n",
      "ultralytics-main/docs/en/help/CI.md\n",
      "ultralytics-main/docs/en/help/CLA.md\n",
      "ultralytics-main/docs/en/help/FAQ.md\n",
      "ultralytics-main/docs/en/help/code-of-conduct.md\n",
      "ultralytics-main/docs/en/help/contributing.md\n",
      "ultralytics-main/docs/en/help/environmental-health-safety.md\n",
      "ultralytics-main/docs/en/help/index.md\n",
      "ultralytics-main/docs/en/help/minimum-reproducible-example.md\n",
      "ultralytics-main/docs/en/help/privacy.md\n",
      "ultralytics-main/docs/en/help/security.md\n",
      "ultralytics-main/docs/en/hub/api/index.md\n",
      "ultralytics-main/docs/en/hub/app/android.md\n",
      "ultralytics-main/docs/en/hub/app/index.md\n",
      "ultralytics-main/docs/en/hub/app/ios.md\n",
      "ultralytics-main/docs/en/hub/cloud-training.md\n",
      "ultralytics-main/docs/en/hub/datasets.md\n",
      "ultralytics-main/docs/en/hub/index.md\n",
      "ultralytics-main/docs/en/hub/inference-api.md\n",
      "ultralytics-main/docs/en/hub/integrations.md\n",
      "ultralytics-main/docs/en/hub/models.md\n",
      "ultralytics-main/docs/en/hub/pro.md\n",
      "ultralytics-main/docs/en/hub/projects.md\n",
      "ultralytics-main/docs/en/hub/quickstart.md\n",
      "ultralytics-main/docs/en/hub/teams.md\n",
      "ultralytics-main/docs/en/index.md\n",
      "ultralytics-main/docs/en/integrations/albumentations.md\n",
      "ultralytics-main/docs/en/integrations/amazon-sagemaker.md\n",
      "ultralytics-main/docs/en/integrations/clearml.md\n",
      "ultralytics-main/docs/en/integrations/comet.md\n",
      "ultralytics-main/docs/en/integrations/coreml.md\n",
      "ultralytics-main/docs/en/integrations/dvc.md\n",
      "ultralytics-main/docs/en/integrations/edge-tpu.md\n",
      "ultralytics-main/docs/en/integrations/google-colab.md\n",
      "ultralytics-main/docs/en/integrations/gradio.md\n",
      "ultralytics-main/docs/en/integrations/ibm-watsonx.md\n",
      "ultralytics-main/docs/en/integrations/index.md\n",
      "ultralytics-main/docs/en/integrations/jupyterlab.md\n",
      "ultralytics-main/docs/en/integrations/kaggle.md\n",
      "ultralytics-main/docs/en/integrations/mlflow.md\n",
      "ultralytics-main/docs/en/integrations/mnn.md\n",
      "ultralytics-main/docs/en/integrations/ncnn.md\n",
      "ultralytics-main/docs/en/integrations/neural-magic.md\n",
      "ultralytics-main/docs/en/integrations/onnx.md\n",
      "ultralytics-main/docs/en/integrations/openvino.md\n",
      "ultralytics-main/docs/en/integrations/paddlepaddle.md\n",
      "ultralytics-main/docs/en/integrations/paperspace.md\n",
      "ultralytics-main/docs/en/integrations/ray-tune.md\n",
      "ultralytics-main/docs/en/integrations/roboflow.md\n",
      "ultralytics-main/docs/en/integrations/rockchip-rknn.md\n",
      "ultralytics-main/docs/en/integrations/seeedstudio-recamera.md\n",
      "ultralytics-main/docs/en/integrations/sony-imx500.md\n",
      "ultralytics-main/docs/en/integrations/tensorboard.md\n",
      "ultralytics-main/docs/en/integrations/tensorrt.md\n",
      "ultralytics-main/docs/en/integrations/tf-graphdef.md\n",
      "ultralytics-main/docs/en/integrations/tf-savedmodel.md\n",
      "ultralytics-main/docs/en/integrations/tfjs.md\n",
      "ultralytics-main/docs/en/integrations/tflite.md\n",
      "ultralytics-main/docs/en/integrations/torchscript.md\n",
      "ultralytics-main/docs/en/integrations/vscode.md\n",
      "ultralytics-main/docs/en/integrations/weights-biases.md\n",
      "ultralytics-main/docs/en/macros/augmentation-args.md\n",
      "ultralytics-main/docs/en/macros/export-args.md\n",
      "ultralytics-main/docs/en/macros/export-table.md\n",
      "ultralytics-main/docs/en/macros/predict-args.md\n",
      "ultralytics-main/docs/en/macros/sam-auto-annotate.md\n",
      "ultralytics-main/docs/en/macros/solutions-args.md\n",
      "ultralytics-main/docs/en/macros/track-args.md\n",
      "ultralytics-main/docs/en/macros/train-args.md\n",
      "ultralytics-main/docs/en/macros/validation-args.md\n",
      "ultralytics-main/docs/en/macros/visualization-args.md\n",
      "ultralytics-main/docs/en/macros/yolo-cls-perf.md\n",
      "ultralytics-main/docs/en/macros/yolo-det-perf.md\n",
      "ultralytics-main/docs/en/macros/yolo-obb-perf.md\n",
      "ultralytics-main/docs/en/macros/yolo-pose-perf.md\n",
      "ultralytics-main/docs/en/macros/yolo-seg-perf.md\n",
      "ultralytics-main/docs/en/models/fast-sam.md\n",
      "ultralytics-main/docs/en/models/index.md\n",
      "ultralytics-main/docs/en/models/mobile-sam.md\n",
      "ultralytics-main/docs/en/models/rtdetr.md\n",
      "ultralytics-main/docs/en/models/sam-2.md\n",
      "ultralytics-main/docs/en/models/sam.md\n",
      "ultralytics-main/docs/en/models/yolo-nas.md\n",
      "ultralytics-main/docs/en/models/yolo-world.md\n",
      "ultralytics-main/docs/en/models/yolo11.md\n",
      "ultralytics-main/docs/en/models/yolo12.md\n",
      "ultralytics-main/docs/en/models/yolo26.md\n",
      "ultralytics-main/docs/en/models/yoloe.md\n",
      "ultralytics-main/docs/en/models/yolov10.md\n",
      "ultralytics-main/docs/en/models/yolov3.md\n",
      "ultralytics-main/docs/en/models/yolov4.md\n",
      "ultralytics-main/docs/en/models/yolov5.md\n",
      "ultralytics-main/docs/en/models/yolov6.md\n",
      "ultralytics-main/docs/en/models/yolov7.md\n",
      "ultralytics-main/docs/en/models/yolov8.md\n",
      "ultralytics-main/docs/en/models/yolov9.md\n",
      "ultralytics-main/docs/en/modes/benchmark.md\n",
      "ultralytics-main/docs/en/modes/export.md\n",
      "ultralytics-main/docs/en/modes/index.md\n",
      "ultralytics-main/docs/en/modes/predict.md\n",
      "ultralytics-main/docs/en/modes/track.md\n",
      "ultralytics-main/docs/en/modes/train.md\n",
      "ultralytics-main/docs/en/modes/val.md\n",
      "ultralytics-main/docs/en/quickstart.md\n",
      "ultralytics-main/docs/en/reference/__init__.md\n",
      "ultralytics-main/docs/en/reference/cfg/__init__.md\n",
      "ultralytics-main/docs/en/reference/data/annotator.md\n",
      "ultralytics-main/docs/en/reference/data/augment.md\n",
      "ultralytics-main/docs/en/reference/data/base.md\n",
      "ultralytics-main/docs/en/reference/data/build.md\n",
      "ultralytics-main/docs/en/reference/data/converter.md\n",
      "ultralytics-main/docs/en/reference/data/dataset.md\n",
      "ultralytics-main/docs/en/reference/data/loaders.md\n",
      "ultralytics-main/docs/en/reference/data/split.md\n",
      "ultralytics-main/docs/en/reference/data/split_dota.md\n",
      "ultralytics-main/docs/en/reference/data/utils.md\n",
      "ultralytics-main/docs/en/reference/engine/exporter.md\n",
      "ultralytics-main/docs/en/reference/engine/model.md\n",
      "ultralytics-main/docs/en/reference/engine/predictor.md\n",
      "ultralytics-main/docs/en/reference/engine/results.md\n",
      "ultralytics-main/docs/en/reference/engine/trainer.md\n",
      "ultralytics-main/docs/en/reference/engine/tuner.md\n",
      "ultralytics-main/docs/en/reference/engine/validator.md\n",
      "ultralytics-main/docs/en/reference/hub/__init__.md\n",
      "ultralytics-main/docs/en/reference/hub/auth.md\n",
      "ultralytics-main/docs/en/reference/hub/google/__init__.md\n",
      "ultralytics-main/docs/en/reference/hub/session.md\n",
      "ultralytics-main/docs/en/reference/hub/utils.md\n",
      "ultralytics-main/docs/en/reference/models/fastsam/model.md\n",
      "ultralytics-main/docs/en/reference/models/fastsam/predict.md\n",
      "ultralytics-main/docs/en/reference/models/fastsam/utils.md\n",
      "ultralytics-main/docs/en/reference/models/fastsam/val.md\n",
      "ultralytics-main/docs/en/reference/models/nas/model.md\n",
      "ultralytics-main/docs/en/reference/models/nas/predict.md\n",
      "ultralytics-main/docs/en/reference/models/nas/val.md\n",
      "ultralytics-main/docs/en/reference/models/rtdetr/model.md\n",
      "ultralytics-main/docs/en/reference/models/rtdetr/predict.md\n",
      "ultralytics-main/docs/en/reference/models/rtdetr/train.md\n",
      "ultralytics-main/docs/en/reference/models/rtdetr/val.md\n",
      "ultralytics-main/docs/en/reference/models/sam/amg.md\n",
      "ultralytics-main/docs/en/reference/models/sam/build.md\n",
      "ultralytics-main/docs/en/reference/models/sam/model.md\n",
      "ultralytics-main/docs/en/reference/models/sam/modules/blocks.md\n",
      "ultralytics-main/docs/en/reference/models/sam/modules/decoders.md\n",
      "ultralytics-main/docs/en/reference/models/sam/modules/encoders.md\n",
      "ultralytics-main/docs/en/reference/models/sam/modules/memory_attention.md\n",
      "ultralytics-main/docs/en/reference/models/sam/modules/sam.md\n",
      "ultralytics-main/docs/en/reference/models/sam/modules/tiny_encoder.md\n",
      "ultralytics-main/docs/en/reference/models/sam/modules/transformer.md\n",
      "ultralytics-main/docs/en/reference/models/sam/modules/utils.md\n",
      "ultralytics-main/docs/en/reference/models/sam/predict.md\n",
      "ultralytics-main/docs/en/reference/models/utils/loss.md\n",
      "ultralytics-main/docs/en/reference/models/utils/ops.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/classify/predict.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/classify/train.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/classify/val.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/detect/predict.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/detect/train.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/detect/val.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/model.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/obb/predict.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/obb/train.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/obb/val.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/pose/predict.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/pose/train.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/pose/val.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/segment/predict.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/segment/train.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/segment/val.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/world/train.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/world/train_world.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/yoloe/predict.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/yoloe/train.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/yoloe/train_seg.md\n",
      "ultralytics-main/docs/en/reference/models/yolo/yoloe/val.md\n",
      "ultralytics-main/docs/en/reference/nn/autobackend.md\n",
      "ultralytics-main/docs/en/reference/nn/modules/activation.md\n",
      "ultralytics-main/docs/en/reference/nn/modules/block.md\n",
      "ultralytics-main/docs/en/reference/nn/modules/conv.md\n",
      "ultralytics-main/docs/en/reference/nn/modules/head.md\n",
      "ultralytics-main/docs/en/reference/nn/modules/transformer.md\n",
      "ultralytics-main/docs/en/reference/nn/modules/utils.md\n",
      "ultralytics-main/docs/en/reference/nn/tasks.md\n",
      "ultralytics-main/docs/en/reference/nn/text_model.md\n",
      "ultralytics-main/docs/en/reference/solutions/ai_gym.md\n",
      "ultralytics-main/docs/en/reference/solutions/analytics.md\n",
      "ultralytics-main/docs/en/reference/solutions/config.md\n",
      "ultralytics-main/docs/en/reference/solutions/distance_calculation.md\n",
      "ultralytics-main/docs/en/reference/solutions/heatmap.md\n",
      "ultralytics-main/docs/en/reference/solutions/instance_segmentation.md\n",
      "ultralytics-main/docs/en/reference/solutions/object_blurrer.md\n",
      "ultralytics-main/docs/en/reference/solutions/object_counter.md\n",
      "ultralytics-main/docs/en/reference/solutions/object_cropper.md\n",
      "ultralytics-main/docs/en/reference/solutions/parking_management.md\n",
      "ultralytics-main/docs/en/reference/solutions/queue_management.md\n",
      "ultralytics-main/docs/en/reference/solutions/region_counter.md\n",
      "ultralytics-main/docs/en/reference/solutions/security_alarm.md\n",
      "ultralytics-main/docs/en/reference/solutions/similarity_search.md\n",
      "ultralytics-main/docs/en/reference/solutions/solutions.md\n",
      "ultralytics-main/docs/en/reference/solutions/speed_estimation.md\n",
      "ultralytics-main/docs/en/reference/solutions/streamlit_inference.md\n",
      "ultralytics-main/docs/en/reference/solutions/trackzone.md\n",
      "ultralytics-main/docs/en/reference/solutions/vision_eye.md\n",
      "ultralytics-main/docs/en/reference/trackers/basetrack.md\n",
      "ultralytics-main/docs/en/reference/trackers/bot_sort.md\n",
      "ultralytics-main/docs/en/reference/trackers/byte_tracker.md\n",
      "ultralytics-main/docs/en/reference/trackers/track.md\n",
      "ultralytics-main/docs/en/reference/trackers/utils/gmc.md\n",
      "ultralytics-main/docs/en/reference/trackers/utils/kalman_filter.md\n",
      "ultralytics-main/docs/en/reference/trackers/utils/matching.md\n",
      "ultralytics-main/docs/en/reference/utils/__init__.md\n",
      "ultralytics-main/docs/en/reference/utils/autobatch.md\n",
      "ultralytics-main/docs/en/reference/utils/autodevice.md\n",
      "ultralytics-main/docs/en/reference/utils/benchmarks.md\n",
      "ultralytics-main/docs/en/reference/utils/callbacks/base.md\n",
      "ultralytics-main/docs/en/reference/utils/callbacks/clearml.md\n",
      "ultralytics-main/docs/en/reference/utils/callbacks/comet.md\n",
      "ultralytics-main/docs/en/reference/utils/callbacks/dvc.md\n",
      "ultralytics-main/docs/en/reference/utils/callbacks/hub.md\n",
      "ultralytics-main/docs/en/reference/utils/callbacks/mlflow.md\n",
      "ultralytics-main/docs/en/reference/utils/callbacks/neptune.md\n",
      "ultralytics-main/docs/en/reference/utils/callbacks/platform.md\n",
      "ultralytics-main/docs/en/reference/utils/callbacks/raytune.md\n",
      "ultralytics-main/docs/en/reference/utils/callbacks/tensorboard.md\n",
      "ultralytics-main/docs/en/reference/utils/callbacks/wb.md\n",
      "ultralytics-main/docs/en/reference/utils/checks.md\n",
      "ultralytics-main/docs/en/reference/utils/cpu.md\n",
      "ultralytics-main/docs/en/reference/utils/dist.md\n",
      "ultralytics-main/docs/en/reference/utils/downloads.md\n",
      "ultralytics-main/docs/en/reference/utils/errors.md\n",
      "ultralytics-main/docs/en/reference/utils/events.md\n",
      "ultralytics-main/docs/en/reference/utils/export/__init__.md\n",
      "ultralytics-main/docs/en/reference/utils/export/imx.md\n",
      "ultralytics-main/docs/en/reference/utils/files.md\n",
      "ultralytics-main/docs/en/reference/utils/git.md\n",
      "ultralytics-main/docs/en/reference/utils/instance.md\n",
      "ultralytics-main/docs/en/reference/utils/logger.md\n",
      "ultralytics-main/docs/en/reference/utils/loss.md\n",
      "ultralytics-main/docs/en/reference/utils/metrics.md\n",
      "ultralytics-main/docs/en/reference/utils/nms.md\n",
      "ultralytics-main/docs/en/reference/utils/ops.md\n",
      "ultralytics-main/docs/en/reference/utils/patches.md\n",
      "ultralytics-main/docs/en/reference/utils/plotting.md\n",
      "ultralytics-main/docs/en/reference/utils/tal.md\n",
      "ultralytics-main/docs/en/reference/utils/torch_utils.md\n",
      "ultralytics-main/docs/en/reference/utils/tqdm.md\n",
      "ultralytics-main/docs/en/reference/utils/triton.md\n",
      "ultralytics-main/docs/en/reference/utils/tuner.md\n",
      "ultralytics-main/docs/en/solutions/index.md\n",
      "ultralytics-main/docs/en/tasks/classify.md\n",
      "ultralytics-main/docs/en/tasks/detect.md\n",
      "ultralytics-main/docs/en/tasks/index.md\n",
      "ultralytics-main/docs/en/tasks/obb.md\n",
      "ultralytics-main/docs/en/tasks/pose.md\n",
      "ultralytics-main/docs/en/tasks/segment.md\n",
      "ultralytics-main/docs/en/usage/callbacks.md\n",
      "ultralytics-main/docs/en/usage/cfg.md\n",
      "ultralytics-main/docs/en/usage/cli.md\n",
      "ultralytics-main/docs/en/usage/engine.md\n",
      "ultralytics-main/docs/en/usage/python.md\n",
      "ultralytics-main/docs/en/usage/simple-utilities.md\n",
      "ultralytics-main/docs/en/yolov5/environments/aws_quickstart_tutorial.md\n",
      "ultralytics-main/docs/en/yolov5/environments/azureml_quickstart_tutorial.md\n",
      "ultralytics-main/docs/en/yolov5/environments/docker_image_quickstart_tutorial.md\n",
      "ultralytics-main/docs/en/yolov5/environments/google_cloud_quickstart_tutorial.md\n",
      "ultralytics-main/docs/en/yolov5/index.md\n",
      "ultralytics-main/docs/en/yolov5/quickstart_tutorial.md\n",
      "ultralytics-main/docs/en/yolov5/tutorials/architecture_description.md\n",
      "ultralytics-main/docs/en/yolov5/tutorials/clearml_logging_integration.md\n",
      "ultralytics-main/docs/en/yolov5/tutorials/comet_logging_integration.md\n",
      "ultralytics-main/docs/en/yolov5/tutorials/hyperparameter_evolution.md\n",
      "ultralytics-main/docs/en/yolov5/tutorials/model_ensembling.md\n",
      "ultralytics-main/docs/en/yolov5/tutorials/model_export.md\n",
      "ultralytics-main/docs/en/yolov5/tutorials/model_pruning_and_sparsity.md\n",
      "ultralytics-main/docs/en/yolov5/tutorials/multi_gpu_training.md\n",
      "ultralytics-main/docs/en/yolov5/tutorials/neural_magic_pruning_quantization.md\n",
      "ultralytics-main/docs/en/yolov5/tutorials/pytorch_hub_model_loading.md\n",
      "ultralytics-main/docs/en/yolov5/tutorials/test_time_augmentation.md\n",
      "ultralytics-main/docs/en/yolov5/tutorials/tips_for_best_training_results.md\n",
      "ultralytics-main/docs/en/yolov5/tutorials/train_custom_data.md\n",
      "ultralytics-main/docs/en/yolov5/tutorials/transfer_learning_with_frozen_layers.md\n",
      "ultralytics-main/examples/README.md\n",
      "ultralytics-main/examples/RTDETR-ONNXRuntime-Python/README.md\n",
      "ultralytics-main/examples/YOLO-Interactive-Tracking-UI/README.md\n",
      "ultralytics-main/examples/YOLO-Series-ONNXRuntime-Rust/README.md\n",
      "ultralytics-main/examples/YOLO11-Triton-CPP/README.md\n",
      "ultralytics-main/examples/YOLOv8-Action-Recognition/README.md\n",
      "ultralytics-main/examples/YOLOv8-CPP-Inference/README.md\n",
      "ultralytics-main/examples/YOLOv8-LibTorch-CPP-Inference/README.md\n",
      "ultralytics-main/examples/YOLOv8-MNN-CPP/README.md\n",
      "ultralytics-main/examples/YOLOv8-ONNXRuntime-CPP/README.md\n",
      "ultralytics-main/examples/YOLOv8-ONNXRuntime-Rust/README.md\n",
      "ultralytics-main/examples/YOLOv8-ONNXRuntime/README.md\n",
      "ultralytics-main/examples/YOLOv8-OpenCV-ONNX-Python/README.md\n",
      "ultralytics-main/examples/YOLOv8-OpenVINO-CPP-Inference/README.md\n",
      "ultralytics-main/examples/YOLOv8-Region-Counter/README.md\n",
      "ultralytics-main/examples/YOLOv8-SAHI-Inference-Video/README.md\n",
      "ultralytics-main/examples/YOLOv8-Segmentation-ONNXRuntime-Python/README.md\n",
      "ultralytics-main/examples/YOLOv8-TFLite-Python/README.md\n",
      "ultralytics-main/ultralytics/cfg/models/README.md\n",
      "ultralytics-main/ultralytics/trackers/README.md\n"
     ]
    }
   ],
   "source": [
    "for data in ultralytics: \n",
    "    print (data[\"filename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab7cea59-39c9-450c-bd55-3aaf9be94368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3314179-1c41-46b9-a143-7a4dddf3c3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ultralytics_chunks =[]\n",
    "\n",
    "for doc in ultralytics:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop(\"content\")\n",
    "    chunks = sliding_window(doc_content, 2000, 1000 )\n",
    "    for chunk in chunks: \n",
    "        chunk.update(doc_copy)\n",
    "    ultralytics_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "377e0c38-a278-40b4-8c77-8fcc2e846b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3382\n"
     ]
    }
   ],
   "source": [
    "print(len(ultralytics_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb18126-d0f0-4b35-bf7a-1baca5483624",
   "metadata": {},
   "source": [
    "we obtained 3382 chunks from 409 documents "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98f1e39-0b83-4383-b3bc-ecb8beb5eb37",
   "metadata": {},
   "source": [
    "# Paragraph chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c61b023e-79da-4dc4-8aaa-3652cc1d2bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = ultralytics[45]['content']\n",
    "paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f03b369-bd8b-48e5-84fa-ac78009a9318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Oriented Bounding Box (OBB) Datasets Overview',\n",
       " 'Training a precise [object detection](https://www.ultralytics.com/glossary/object-detection) model with oriented bounding boxes (OBB) requires a thorough dataset. This guide explains the various OBB dataset formats compatible with Ultralytics YOLO models, offering insights into their structure, application, and methods for format conversions.',\n",
       " '## Supported OBB Dataset Formats',\n",
       " '### YOLO OBB Format',\n",
       " 'The YOLO OBB format designates bounding boxes by their four corner points with coordinates normalized between 0 and 1. It follows this format:',\n",
       " '```bash\\nclass_index x1 y1 x2 y2 x3 y3 x4 y4\\n```',\n",
       " \"Internally, YOLO processes losses and outputs in the `xywhr` format, which represents the [bounding box](https://www.ultralytics.com/glossary/bounding-box)'s center point (xy), width, height, and rotation.\",\n",
       " '<p align=\"center\"><img width=\"800\" src=\"https://github.com/ultralytics/docs/releases/download/0/obb-format-examples.avif\" alt=\"OBB format examples\"></p>',\n",
       " 'An example of a `*.txt` label file for the above image, which contains an object of class `0` in OBB format, could look like:',\n",
       " '```bash\\n0 0.780811 0.743961 0.782371 0.74686 0.777691 0.752174 0.776131 0.749758\\n```',\n",
       " '### Dataset YAML format',\n",
       " 'The Ultralytics framework uses a YAML file format to define the dataset and model configuration for training OBB Models. Here is an example of the YAML format used for defining a OBB dataset:',\n",
       " '!!! example \"ultralytics/cfg/datasets/dota8.yaml\"',\n",
       " '    ```yaml\\n    --8<-- \"ultralytics/cfg/datasets/dota8.yaml\"\\n    ```',\n",
       " '## Usage',\n",
       " 'To train a model using these OBB formats:',\n",
       " '!!! example',\n",
       " '    === \"Python\"',\n",
       " '        ```python\\n        from ultralytics import YOLO',\n",
       " '        # Create a new YOLO11n-OBB model from scratch\\n        model = YOLO(\"yolo11n-obb.yaml\")',\n",
       " '        # Train the model on the DOTAv1 dataset\\n        results = model.train(data=\"DOTAv1.yaml\", epochs=100, imgsz=1024)\\n        ```',\n",
       " '    === \"CLI\"',\n",
       " '        ```bash\\n        # Train a new YOLO11n-OBB model on the DOTAv1 dataset\\n        yolo obb train data=DOTAv1.yaml model=yolo11n-obb.pt epochs=100 imgsz=1024\\n        ```',\n",
       " '## Supported Datasets',\n",
       " 'Currently, the following datasets with Oriented Bounding Boxes are supported:',\n",
       " '- [DOTA-v1](dota-v2.md): The first version of the DOTA dataset, providing a comprehensive set of aerial images with oriented bounding boxes for object detection.\\n- [DOTA-v1.5](dota-v2.md): An intermediate version of the DOTA dataset, offering additional annotations and improvements over DOTA-v1 for enhanced object detection tasks.\\n- [DOTA-v2](dota-v2.md): DOTA (A Large-scale Dataset for Object Detection in Aerial Images) version 2, emphasizes detection from aerial perspectives and contains oriented bounding boxes with 1.7 million instances and 11,268 images.\\n- [DOTA8](dota8.md): A small, 8-image subset of the full DOTA dataset suitable for testing workflows and Continuous Integration (CI) checks of OBB training in the `ultralytics` repository.',\n",
       " '### Incorporating your own OBB dataset',\n",
       " 'For those looking to introduce their own datasets with oriented bounding boxes, ensure compatibility with the \"YOLO OBB format\" mentioned above. Convert your annotations to this required format and detail the paths, classes, and class names in a corresponding YAML configuration file.',\n",
       " '## Convert Label Formats',\n",
       " '### DOTA Dataset Format to YOLO OBB Format',\n",
       " 'Transitioning labels from the DOTA dataset format to the YOLO OBB format can be achieved with this script:',\n",
       " '!!! example',\n",
       " '    === \"Python\"',\n",
       " '        ```python\\n        from ultralytics.data.converter import convert_dota_to_yolo_obb',\n",
       " '        convert_dota_to_yolo_obb(\"path/to/DOTA\")\\n        ```',\n",
       " 'This conversion mechanism is instrumental for datasets in the DOTA format, ensuring alignment with the [Ultralytics YOLO](../../models/yolo11.md) OBB format.',\n",
       " \"It's imperative to validate the compatibility of the dataset with your model and adhere to the necessary format conventions. Properly structured datasets are pivotal for training efficient object detection models with oriented bounding boxes.\",\n",
       " '## FAQ',\n",
       " '### What are Oriented Bounding Boxes (OBB) and how are they used in Ultralytics YOLO models?',\n",
       " 'Oriented Bounding Boxes (OBB) are a type of bounding box annotation where the box can be rotated to align more closely with the object being detected, rather than just being axis-aligned. This is particularly useful in aerial or satellite imagery where objects might not be aligned with the image axes. In [Ultralytics YOLO](../../tasks/obb.md) models, OBBs are represented by their four corner points in the YOLO OBB format. This allows for more accurate object detection since the bounding boxes can rotate to fit the objects better.',\n",
       " '### How do I convert my existing DOTA dataset labels to YOLO OBB format for use with Ultralytics YOLO11?',\n",
       " \"You can convert DOTA dataset labels to YOLO OBB format using the [`convert_dota_to_yolo_obb`](../../reference/data/converter.md) function from Ultralytics. This conversion ensures compatibility with the Ultralytics YOLO models, enabling you to leverage the OBB capabilities for enhanced object detection. Here's a quick example:\",\n",
       " '```python\\nfrom ultralytics.data.converter import convert_dota_to_yolo_obb',\n",
       " 'convert_dota_to_yolo_obb(\"path/to/DOTA\")\\n```',\n",
       " 'This script will reformat your DOTA annotations into a YOLO-compatible format.',\n",
       " '### How do I train a YOLO11 model with oriented bounding boxes (OBB) on my dataset?',\n",
       " \"Training a YOLO11 model with OBBs involves ensuring your dataset is in the YOLO OBB format and then using the [Ultralytics API](../../usage/python.md) to train the model. Here's an example in both Python and CLI:\",\n",
       " '!!! example',\n",
       " '    === \"Python\"',\n",
       " '        ```python\\n        from ultralytics import YOLO',\n",
       " '        # Create a new YOLO11n-OBB model from scratch\\n        model = YOLO(\"yolo11n-obb.yaml\")',\n",
       " '        # Train the model on the custom dataset\\n        results = model.train(data=\"your_dataset.yaml\", epochs=100, imgsz=640)\\n        ```',\n",
       " '    === \"CLI\"',\n",
       " '        ```bash\\n        # Train a new YOLO11n-OBB model on the custom dataset\\n        yolo obb train data=your_dataset.yaml model=yolo11n-obb.yaml epochs=100 imgsz=640\\n        ```',\n",
       " 'This ensures your model leverages the detailed OBB annotations for improved detection [accuracy](https://www.ultralytics.com/glossary/accuracy).',\n",
       " '### What datasets are currently supported for OBB training in Ultralytics YOLO models?',\n",
       " 'Currently, Ultralytics supports the following datasets for OBB training:',\n",
       " '- [DOTA-v1](dota-v2.md): The first version of the DOTA dataset, providing a comprehensive set of aerial images with oriented bounding boxes for object detection.\\n- [DOTA-v1.5](dota-v2.md): An intermediate version of the DOTA dataset, offering additional annotations and improvements over DOTA-v1 for enhanced object detection tasks.\\n- [DOTA-v2](dota-v2.md): This dataset includes 1.7 million instances with oriented bounding boxes and 11,268 images, primarily focusing on aerial object detection.\\n- [DOTA8](dota8.md): A smaller, 8-image subset of the DOTA dataset used for testing and [continuous integration](../../help/CI.md) (CI) checks.',\n",
       " 'These datasets are tailored for scenarios where OBBs offer a significant advantage, such as aerial and satellite image analysis.',\n",
       " '### Can I use my own dataset with oriented bounding boxes for YOLO11 training, and if so, how?',\n",
       " 'Yes, you can use your own dataset with oriented bounding boxes for YOLO11 training. Ensure your dataset annotations are converted to the YOLO OBB format, which involves defining bounding boxes by their four corner points. You can then create a [YAML configuration file](../../usage/cfg.md) specifying the dataset paths, classes, and other necessary details. For more information on creating and configuring your datasets, refer to the [Supported Datasets](#supported-datasets) section.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389994e6-2bc1-4756-b26b-d14844f791ff",
   "metadata": {},
   "source": [
    "# chunking by sections and headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a52234c-3d7f-4c9c-b10e-30fe4c89b193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_markdown_by_level(text, level=2):\n",
    "    \"\"\"\n",
    "    Split markdown text by a specific header level.\n",
    "    \n",
    "    :param text: Markdown text as a string\n",
    "    :param level: Header level to split on\n",
    "    :return: List of sections as strings\n",
    "    \"\"\"\n",
    "    # This regex matches markdown headers\n",
    "    # For level 2, it matches lines starting with \"## \"\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "    # Split and keep the headers\n",
    "    parts = pattern.split(text)\n",
    "    \n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        # We step by 3 because regex.split() with\n",
    "        # capturing groups returns:\n",
    "        # [before_match, group1, group2, after_match, ...]\n",
    "        # here group1 is \"## \", group2 is the header text\n",
    "        header = parts[i] + parts[i+1]  # \"## \" + \"Title\"\n",
    "        header = header.strip()\n",
    "\n",
    "        # Get the content after this header\n",
    "        content = \"\"\n",
    "        if i+2 < len(parts):\n",
    "            content = parts[i+2].strip()\n",
    "\n",
    "        if content:\n",
    "            section = f'{header}\\n\\n{content}'\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "    \n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fe462e5-1483-46e0-9b6d-63294e3e0f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ultralytics_chunks2 = []\n",
    "\n",
    "for doc in ultralytics:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        ultralytics_chunks2.append(section_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f3818bc-1e7a-4157-9f1e-3cdf10bdcaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2459\n"
     ]
    }
   ],
   "source": [
    "print(len(ultralytics_chunks2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553677d9-0665-4c6b-96da-0e96f6faf47b",
   "metadata": {},
   "source": [
    "# Intelligent chunking "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a12fe1-08ae-4df2-b7b6-790be3a6850f",
   "metadata": {},
   "source": [
    "This process requires time and incurs costs. As mentioned before, use this only when really necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25f54027-845c-44da-aba1-0e18318b970e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71bc9202-7883-4b73-b873-8f89d1271c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# openai_client = OpenAI()\n",
    "\n",
    "\n",
    "def llm(prompt, model='gpt-4o-mini'):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model='gpt-4o-mini',\n",
    "        input=messages\n",
    "    )\n",
    "\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5702322d-4ebe-48b3-8123-ec2f5f0b15d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec85d918-52f6-4a60-96bf-08985976e689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intelligent_chunking(text):\n",
    "    prompt = prompt_template.format(document=text)\n",
    "    response = llm(prompt)\n",
    "    sections = response.split('---')\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc6dae44-dce7-4c7b-813f-a2b00ff48321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7363bc25501e40d4a37c8b5292468f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ultralytics_chunks3 = []\n",
    "for doc in tqdm(ultralytics_chunks3):\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        ultralytics_chunks3.append(section_doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09577bf5-a34b-4445-b850-ea516d6ee9c6",
   "metadata": {},
   "source": [
    "# Day 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcde3593-21b1-4a2a-8086-5a4f3bb0209f",
   "metadata": {},
   "source": [
    "# Text search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fe5fd5e-5111-4ed5-beda-4e2ec76b12ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x1e63d533190>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import Index \n",
    "\n",
    "index = Index(text_fields = [\"chunk\", \"title\", \"description\",\"filename\"], keyword_fields = [])\n",
    "index.fit(ultralytics_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5281d74-5e37-44e7-8247-8cd5208517b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what should be in a test dataset for AI evaluation?\"\n",
    "results = index.search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c826ba7-f60a-481b-a7b8-483b62fb8c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0,\n",
       "  'chunk': '# Reference for `ultralytics/data/dataset.py`\\n\\n!!! note\\n\\n    This file is available at [https://github.com/ultralytics/ultralytics/blob/main/ultralytics/data/dataset.py](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/data/dataset.py). If you spot a problem please help fix it by [contributing](https://docs.ultralytics.com/help/contributing/) a [Pull Request](https://github.com/ultralytics/ultralytics/edit/main/ultralytics/data/dataset.py) üõ†Ô∏è. Thank you üôè!\\n\\n<br>\\n\\n## ::: ultralytics.data.dataset.YOLODataset\\n\\n<br><br><hr><br>\\n\\n## ::: ultralytics.data.dataset.YOLOMultiModalDataset\\n\\n<br><br><hr><br>\\n\\n## ::: ultralytics.data.dataset.GroundingDataset\\n\\n<br><br><hr><br>\\n\\n## ::: ultralytics.data.dataset.YOLOConcatDataset\\n\\n<br><br><hr><br>\\n\\n## ::: ultralytics.data.dataset.SemanticDataset\\n\\n<br><br><hr><br>\\n\\n## ::: ultralytics.data.dataset.ClassificationDataset\\n\\n<br><br>',\n",
       "  'description': 'Explore the YOLODataset and its subclasses for object detection, segmentation, and multi-modal tasks. Find details on dataset loading, caching, and augmentation.',\n",
       "  'keywords': 'Ultralytics, YOLODataset, object detection, segmentation, dataset loading, caching, data augmentation',\n",
       "  'filename': 'ultralytics-main/docs/en/reference/data/dataset.md'},\n",
       " {'start': 12000,\n",
       "  'chunk': \" more about these metrics in our [YOLO11 performance metrics guide](./yolo-performance-metrics.md).\\n\\n### How can I fine-tune a pre-trained YOLO11 model for my specific dataset?\\n\\nFine-tuning a pre-trained YOLO11 model involves adjusting its parameters to improve performance on a specific task or dataset. Start by evaluating your model using metrics, then set a higher initial learning rate by adjusting the `warmup_epochs` parameter to 0 for immediate stability. Use parameters like `rect=true` for handling varied image sizes effectively. For more detailed guidance, refer to our section on [fine-tuning YOLO11 models](#how-does-fine-tuning-work).\\n\\n### How can I handle variable image sizes when evaluating my YOLO11 model?\\n\\nTo handle variable image sizes during evaluation, use the `rect=true` parameter in YOLO11, which adjusts the network's stride for each batch based on image sizes. The `imgsz` parameter sets the maximum dimension for image resizing, defaulting to 640. Adjust `imgsz` to suit your dataset and GPU memory. For more details, visit our [section on handling variable image sizes](#handling-variable-image-sizes).\\n\\n### What practical steps can I take to improve mean average precision for my YOLO11 model?\\n\\nImproving mean average precision (mAP) for a YOLO11 model involves several steps:\\n\\n1. **Tuning Hyperparameters**: Experiment with different learning rates, [batch sizes](https://www.ultralytics.com/glossary/batch-size), and image augmentations.\\n2. **[Data Augmentation](https://www.ultralytics.com/glossary/data-augmentation)**: Use techniques like Mosaic and MixUp to create diverse training samples.\\n3. **Image Tiling**: Split larger images into smaller tiles to improve detection accuracy for small objects.\\n   Refer to our detailed guide on [model fine-tuning](#tips-for-fine-tuning-your-model) for specific strategies.\\n\\n### How do I access YOLO11 model evaluation metrics in Python?\\n\\nYou can access YOLO11 model evaluation metrics using Python with the following steps\",\n",
       "  'comments': True,\n",
       "  'description': 'Explore the most effective ways to assess and refine YOLO11 models for better performance. Learn about evaluation metrics, fine-tuning processes, and how to customize your model for specific needs.',\n",
       "  'keywords': 'Model Evaluation, Machine Learning Model Evaluation, Fine Tuning Machine Learning, Fine Tune Model, Evaluating Models, Model Fine Tuning, How to Fine Tune a Model',\n",
       "  'filename': 'ultralytics-main/docs/en/guides/model-evaluation-insights.md'},\n",
       " {'start': 13000,\n",
       "  'chunk': ' your dataset and GPU memory. For more details, visit our [section on handling variable image sizes](#handling-variable-image-sizes).\\n\\n### What practical steps can I take to improve mean average precision for my YOLO11 model?\\n\\nImproving mean average precision (mAP) for a YOLO11 model involves several steps:\\n\\n1. **Tuning Hyperparameters**: Experiment with different learning rates, [batch sizes](https://www.ultralytics.com/glossary/batch-size), and image augmentations.\\n2. **[Data Augmentation](https://www.ultralytics.com/glossary/data-augmentation)**: Use techniques like Mosaic and MixUp to create diverse training samples.\\n3. **Image Tiling**: Split larger images into smaller tiles to improve detection accuracy for small objects.\\n   Refer to our detailed guide on [model fine-tuning](#tips-for-fine-tuning-your-model) for specific strategies.\\n\\n### How do I access YOLO11 model evaluation metrics in Python?\\n\\nYou can access YOLO11 model evaluation metrics using Python with the following steps:\\n\\n!!! example \"Usage\"\\n\\n    === \"Python\"\\n\\n        ```python\\n        from ultralytics import YOLO\\n\\n        # Load the model\\n        model = YOLO(\"yolo11n.pt\")\\n\\n        # Run the evaluation\\n        results = model.val(data=\"coco8.yaml\")\\n\\n        # Print specific metrics\\n        print(\"Class indices with average precision:\", results.ap_class_index)\\n        print(\"Average precision for all classes:\", results.box.all_ap)\\n        print(\"Mean average precision at IoU=0.50:\", results.box.map50)\\n        print(\"Mean recall:\", results.box.mr)\\n        ```\\n\\nAnalyzing these metrics helps fine-tune and optimize your YOLO11 model. For a deeper dive, check out our guide on [YOLO11 metrics](../modes/val.md).',\n",
       "  'comments': True,\n",
       "  'description': 'Explore the most effective ways to assess and refine YOLO11 models for better performance. Learn about evaluation metrics, fine-tuning processes, and how to customize your model for specific needs.',\n",
       "  'keywords': 'Model Evaluation, Machine Learning Model Evaluation, Fine Tuning Machine Learning, Fine Tune Model, Evaluating Models, Model Fine Tuning, How to Fine Tune a Model',\n",
       "  'filename': 'ultralytics-main/docs/en/guides/model-evaluation-insights.md'},\n",
       " {'start': 0,\n",
       "  'chunk': '# Insights on Model Evaluation and Fine-Tuning\\n\\n## Introduction\\n\\nOnce you\\'ve [trained](./model-training-tips.md) your computer vision model, evaluating and refining it to perform optimally is essential. Just training your model isn\\'t enough. You need to make sure that your model is accurate, efficient, and fulfills the [objective](./defining-project-goals.md) of your computer vision project. By evaluating and fine-tuning your model, you can identify weaknesses, improve its accuracy, and boost overall performance.\\n\\n<p align=\"center\">\\n  <br>\\n  <iframe loading=\"lazy\" width=\"720\" height=\"405\" src=\"https://www.youtube.com/embed/-aYO-6VaDrw\"\\n    title=\"YouTube video player\" frameborder=\"0\"\\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\\n    allowfullscreen>\\n  </iframe>\\n  <br>\\n  <strong>Watch:</strong> Insights into Model Evaluation and Fine-Tuning | Tips for Improving Mean Average Precision\\n</p>\\n\\nIn this guide, we\\'ll share insights on model evaluation and fine-tuning that\\'ll make this [step of a computer vision project](./steps-of-a-cv-project.md) more approachable. We\\'ll discuss how to understand evaluation metrics and implement fine-tuning techniques, giving you the knowledge to elevate your model\\'s capabilities.\\n\\n## Evaluating Model Performance Using Metrics\\n\\nEvaluating how well a model performs helps us understand how effectively it works. Various metrics are used to measure performance. These [performance metrics](./yolo-performance-metrics.md) provide clear, numerical insights that can guide improvements toward making sure the model meets its intended goals. Let\\'s take a closer look at a few key metrics.\\n\\n### Confidence Score\\n\\nThe confidence score represents the model\\'s certainty that a detected object belongs to a particular class. It ranges from 0 to 1, with higher scores indicating greater confidence. The confidence score helps filter predictions; only detections with confidence scores above a specif',\n",
       "  'comments': True,\n",
       "  'description': 'Explore the most effective ways to assess and refine YOLO11 models for better performance. Learn about evaluation metrics, fine-tuning processes, and how to customize your model for specific needs.',\n",
       "  'keywords': 'Model Evaluation, Machine Learning Model Evaluation, Fine Tuning Machine Learning, Fine Tune Model, Evaluating Models, Model Fine Tuning, How to Fine Tune a Model',\n",
       "  'filename': 'ultralytics-main/docs/en/guides/model-evaluation-insights.md'},\n",
       " {'start': 11000,\n",
       "  'chunk': \"important steps for successful [model deployment](https://www.ultralytics.com/glossary/model-deployment). These steps help make sure that your model is accurate, efficient, and suited to your overall application. The key to training the best model possible is continuous experimentation and learning. Don't hesitate to tweak parameters, try new techniques, and explore different datasets. Keep experimenting and pushing the boundaries of what's possible!\\n\\n## FAQ\\n\\n### What are the key metrics for evaluating YOLO11 model performance?\\n\\nTo evaluate YOLO11 model performance, important metrics include Confidence Score, Intersection over Union (IoU), and Mean Average Precision (mAP). Confidence Score measures the model's certainty for each detected object class. IoU evaluates how well the predicted bounding box overlaps with the ground truth. Mean Average Precision (mAP) aggregates precision scores across classes, with mAP@.5 and mAP@.5:.95 being two common types for varying IoU thresholds. Learn more about these metrics in our [YOLO11 performance metrics guide](./yolo-performance-metrics.md).\\n\\n### How can I fine-tune a pre-trained YOLO11 model for my specific dataset?\\n\\nFine-tuning a pre-trained YOLO11 model involves adjusting its parameters to improve performance on a specific task or dataset. Start by evaluating your model using metrics, then set a higher initial learning rate by adjusting the `warmup_epochs` parameter to 0 for immediate stability. Use parameters like `rect=true` for handling varied image sizes effectively. For more detailed guidance, refer to our section on [fine-tuning YOLO11 models](#how-does-fine-tuning-work).\\n\\n### How can I handle variable image sizes when evaluating my YOLO11 model?\\n\\nTo handle variable image sizes during evaluation, use the `rect=true` parameter in YOLO11, which adjusts the network's stride for each batch based on image sizes. The `imgsz` parameter sets the maximum dimension for image resizing, defaulting to 640. Adjust `imgsz` to suit\",\n",
       "  'comments': True,\n",
       "  'description': 'Explore the most effective ways to assess and refine YOLO11 models for better performance. Learn about evaluation metrics, fine-tuning processes, and how to customize your model for specific needs.',\n",
       "  'keywords': 'Model Evaluation, Machine Learning Model Evaluation, Fine Tuning Machine Learning, Fine Tune Model, Evaluating Models, Model Fine Tuning, How to Fine Tune a Model',\n",
       "  'filename': 'ultralytics-main/docs/en/guides/model-evaluation-insights.md'},\n",
       " {'start': 5000,\n",
       "  'chunk': 'd.\\n\\n### Common Community Questions\\n\\nWhen evaluating your YOLO11 model, you might run into a few hiccups. Based on common community questions, here are some tips to help you get the most out of your YOLO11 model:\\n\\n#### Handling Variable Image Sizes\\n\\nEvaluating your YOLO11 model with images of different sizes can help you understand its performance on diverse datasets. Using the `rect=true` validation parameter, YOLO11 adjusts the network\\'s stride for each batch based on the image sizes, allowing the model to handle rectangular images without forcing them to a single size.\\n\\nThe `imgsz` validation parameter sets the maximum dimension for image resizing, which is 640 by default. You can adjust this based on your dataset\\'s maximum dimensions and the GPU memory available. Even with `imgsz` set, `rect=true` lets the model manage varying image sizes effectively by dynamically adjusting the stride.\\n\\n#### Accessing YOLO11 Metrics\\n\\nIf you want to get a deeper understanding of your YOLO11 model\\'s performance, you can easily access specific evaluation metrics with a few lines of Python code. The code snippet below will let you load your model, run an evaluation, and print out various metrics that show how well your model is doing.\\n\\n!!! example \"Usage\"\\n\\n    === \"Python\"\\n\\n        ```python\\n        from ultralytics import YOLO\\n\\n        # Load the model\\n        model = YOLO(\"yolo11n.pt\")\\n\\n        # Run the evaluation\\n        results = model.val(data=\"coco8.yaml\")\\n\\n        # Print specific metrics\\n        print(\"Class indices with average precision:\", results.ap_class_index)\\n        print(\"Average precision for all classes:\", results.box.all_ap)\\n        print(\"Average precision:\", results.box.ap)\\n        print(\"Average precision at IoU=0.50:\", results.box.ap50)\\n        print(\"Class indices for average precision:\", results.box.ap_class_index)\\n        print(\"Class-specific results:\", results.box.class_result)\\n        print(\"F1 score:\", results.box.f1)\\n        print(\"F1 score curve:\", r',\n",
       "  'comments': True,\n",
       "  'description': 'Explore the most effective ways to assess and refine YOLO11 models for better performance. Learn about evaluation metrics, fine-tuning processes, and how to customize your model for specific needs.',\n",
       "  'keywords': 'Model Evaluation, Machine Learning Model Evaluation, Fine Tuning Machine Learning, Fine Tune Model, Evaluating Models, Model Fine Tuning, How to Fine Tune a Model',\n",
       "  'filename': 'ultralytics-main/docs/en/guides/model-evaluation-insights.md'},\n",
       " {'start': 8000,\n",
       "  'chunk': 'e metrics, you can fine-tune and optimize your YOLO11 model for better performance, making it more effective for your specific use case.\\n\\n## How Does Fine-Tuning Work?\\n\\nFine-tuning involves taking a pre-trained model and adjusting its parameters to improve performance on a specific task or dataset. The process, also known as model retraining, allows the model to better understand and predict outcomes for the specific data it will encounter in real-world applications. You can retrain your model based on your model evaluation to achieve optimal results.\\n\\n## Tips for Fine-Tuning Your Model\\n\\nFine-tuning a model means paying close attention to several vital parameters and techniques to achieve optimal performance. Here are some essential tips to guide you through the process.\\n\\n### Starting With a Higher Learning Rate\\n\\nUsually, during the initial training [epochs](https://www.ultralytics.com/glossary/epoch), the learning rate starts low and gradually increases to stabilize the training process. However, since your model has already learned some features from the previous dataset, starting with a higher [learning rate](https://www.ultralytics.com/glossary/learning-rate) right away can be more beneficial.\\n\\nWhen evaluating your YOLO11 model, you can set the `warmup_epochs` validation parameter to `warmup_epochs=0` to prevent the learning rate from starting too high. By following this process, the training will continue from the provided weights, adjusting to the nuances of your new data.\\n\\n### Image Tiling for Small Objects\\n\\nImage tiling can improve detection accuracy for small objects. By dividing larger images into smaller segments, such as splitting 1280x1280 images into multiple 640x640 segments, you maintain the original resolution, and the model can learn from high-resolution fragments. When using YOLO11, make sure to adjust your labels for these new segments correctly.\\n\\n## Engage with the Community\\n\\nSharing your ideas and questions with other [computer vision](https://',\n",
       "  'comments': True,\n",
       "  'description': 'Explore the most effective ways to assess and refine YOLO11 models for better performance. Learn about evaluation metrics, fine-tuning processes, and how to customize your model for specific needs.',\n",
       "  'keywords': 'Model Evaluation, Machine Learning Model Evaluation, Fine Tuning Machine Learning, Fine Tune Model, Evaluating Models, Model Fine Tuning, How to Fine Tune a Model',\n",
       "  'filename': 'ultralytics-main/docs/en/guides/model-evaluation-insights.md'},\n",
       " {'start': 1000,\n",
       "  'chunk': \"hts on model evaluation and fine-tuning that'll make this [step of a computer vision project](./steps-of-a-cv-project.md) more approachable. We'll discuss how to understand evaluation metrics and implement fine-tuning techniques, giving you the knowledge to elevate your model's capabilities.\\n\\n## Evaluating Model Performance Using Metrics\\n\\nEvaluating how well a model performs helps us understand how effectively it works. Various metrics are used to measure performance. These [performance metrics](./yolo-performance-metrics.md) provide clear, numerical insights that can guide improvements toward making sure the model meets its intended goals. Let's take a closer look at a few key metrics.\\n\\n### Confidence Score\\n\\nThe confidence score represents the model's certainty that a detected object belongs to a particular class. It ranges from 0 to 1, with higher scores indicating greater confidence. The confidence score helps filter predictions; only detections with confidence scores above a specified threshold are considered valid.\\n\\n_Quick Tip:_ When running inferences, if you aren't seeing any predictions, and you've checked everything else, try lowering the confidence score. Sometimes, the threshold is too high, causing the model to ignore valid predictions. Lowering the score allows the model to consider more possibilities. This might not meet your project goals, but it's a good way to see what the model can do and decide how to fine-tune it.\\n\\n### Intersection over Union\\n\\n[Intersection over Union](https://www.ultralytics.com/glossary/intersection-over-union-iou) (IoU) is a metric in [object detection](https://www.ultralytics.com/glossary/object-detection) that measures how well the predicted [bounding box](https://www.ultralytics.com/glossary/bounding-box) overlaps with the ground truth bounding box. IoU values range from 0 to 1, where one stands for a perfect match. IoU is essential because it measures how closely the predicted boundaries match the actual object boundaries.\",\n",
       "  'comments': True,\n",
       "  'description': 'Explore the most effective ways to assess and refine YOLO11 models for better performance. Learn about evaluation metrics, fine-tuning processes, and how to customize your model for specific needs.',\n",
       "  'keywords': 'Model Evaluation, Machine Learning Model Evaluation, Fine Tuning Machine Learning, Fine Tune Model, Evaluating Models, Model Fine Tuning, How to Fine Tune a Model',\n",
       "  'filename': 'ultralytics-main/docs/en/guides/model-evaluation-insights.md'},\n",
       " {'start': 6000,\n",
       "  'chunk': 'performance, you can easily access specific evaluation metrics with a few lines of Python code. The code snippet below will let you load your model, run an evaluation, and print out various metrics that show how well your model is doing.\\n\\n!!! example \"Usage\"\\n\\n    === \"Python\"\\n\\n        ```python\\n        from ultralytics import YOLO\\n\\n        # Load the model\\n        model = YOLO(\"yolo11n.pt\")\\n\\n        # Run the evaluation\\n        results = model.val(data=\"coco8.yaml\")\\n\\n        # Print specific metrics\\n        print(\"Class indices with average precision:\", results.ap_class_index)\\n        print(\"Average precision for all classes:\", results.box.all_ap)\\n        print(\"Average precision:\", results.box.ap)\\n        print(\"Average precision at IoU=0.50:\", results.box.ap50)\\n        print(\"Class indices for average precision:\", results.box.ap_class_index)\\n        print(\"Class-specific results:\", results.box.class_result)\\n        print(\"F1 score:\", results.box.f1)\\n        print(\"F1 score curve:\", results.box.f1_curve)\\n        print(\"Overall fitness score:\", results.box.fitness)\\n        print(\"Mean average precision:\", results.box.map)\\n        print(\"Mean average precision at IoU=0.50:\", results.box.map50)\\n        print(\"Mean average precision at IoU=0.75:\", results.box.map75)\\n        print(\"Mean average precision for different IoU thresholds:\", results.box.maps)\\n        print(\"Mean results for different metrics:\", results.box.mean_results)\\n        print(\"Mean precision:\", results.box.mp)\\n        print(\"Mean recall:\", results.box.mr)\\n        print(\"Precision:\", results.box.p)\\n        print(\"Precision curve:\", results.box.p_curve)\\n        print(\"Precision values:\", results.box.prec_values)\\n        print(\"Specific precision metrics:\", results.box.px)\\n        print(\"Recall:\", results.box.r)\\n        print(\"Recall curve:\", results.box.r_curve)\\n        ```\\n\\nThe results object also includes speed metrics like preprocess time, inference time, loss, and postprocess time. By analyzing thes',\n",
       "  'comments': True,\n",
       "  'description': 'Explore the most effective ways to assess and refine YOLO11 models for better performance. Learn about evaluation metrics, fine-tuning processes, and how to customize your model for specific needs.',\n",
       "  'keywords': 'Model Evaluation, Machine Learning Model Evaluation, Fine Tuning Machine Learning, Fine Tune Model, Evaluating Models, Model Fine Tuning, How to Fine Tune a Model',\n",
       "  'filename': 'ultralytics-main/docs/en/guides/model-evaluation-insights.md'},\n",
       " {'start': 10000,\n",
       "  'chunk': \"www.ultralytics.com/glossary/computer-vision-cv) enthusiasts can inspire creative solutions to roadblocks in your projects. Here are some excellent ways to learn, troubleshoot, and connect.\\n\\n### Finding Help and Support\\n\\n- **GitHub Issues:** Explore the YOLO11 GitHub repository and use the [Issues tab](https://github.com/ultralytics/ultralytics/issues) to ask questions, report bugs, and suggest features. The community and maintainers are available to assist with any issues you encounter.\\n- **Ultralytics Discord Server:** Join the [Ultralytics Discord server](https://discord.com/invite/ultralytics) to connect with other users and developers, get support, share knowledge, and brainstorm ideas.\\n\\n### Official Documentation\\n\\n- **Ultralytics YOLO11 Documentation:** Check out the [official YOLO11 documentation](./index.md) for comprehensive guides and valuable insights on various computer vision tasks and projects.\\n\\n## Final Thoughts\\n\\nEvaluating and fine-tuning your computer vision model are important steps for successful [model deployment](https://www.ultralytics.com/glossary/model-deployment). These steps help make sure that your model is accurate, efficient, and suited to your overall application. The key to training the best model possible is continuous experimentation and learning. Don't hesitate to tweak parameters, try new techniques, and explore different datasets. Keep experimenting and pushing the boundaries of what's possible!\\n\\n## FAQ\\n\\n### What are the key metrics for evaluating YOLO11 model performance?\\n\\nTo evaluate YOLO11 model performance, important metrics include Confidence Score, Intersection over Union (IoU), and Mean Average Precision (mAP). Confidence Score measures the model's certainty for each detected object class. IoU evaluates how well the predicted bounding box overlaps with the ground truth. Mean Average Precision (mAP) aggregates precision scores across classes, with mAP@.5 and mAP@.5:.95 being two common types for varying IoU thresholds. Learn\",\n",
       "  'comments': True,\n",
       "  'description': 'Explore the most effective ways to assess and refine YOLO11 models for better performance. Learn about evaluation metrics, fine-tuning processes, and how to customize your model for specific needs.',\n",
       "  'keywords': 'Model Evaluation, Machine Learning Model Evaluation, Fine Tuning Machine Learning, Fine Tune Model, Evaluating Models, Model Fine Tuning, How to Fine Tune a Model',\n",
       "  'filename': 'ultralytics-main/docs/en/guides/model-evaluation-insights.md'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251aee55-4014-41b2-8e23-6355e63a8f6f",
   "metadata": {},
   "source": [
    "# Vector Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c3b4e62-7efa-43e6-84cc-04557d01d5e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9ea69072f246d695613213237eb242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3382 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'VectorSearch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m     ultralytics_embeddings\u001b[38;5;241m.\u001b[39mappend(v)\n\u001b[0;32m      9\u001b[0m ultralytics_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(ultralytics_embeddings)\n\u001b[1;32m---> 10\u001b[0m ultralytics_vindex \u001b[38;5;241m=\u001b[39m \u001b[43mVectorSearch\u001b[49m()\n\u001b[0;32m     11\u001b[0m ultralytics_vindex\u001b[38;5;241m.\u001b[39mfit(ultralytics_embeddings, ultralytics_chunks)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'VectorSearch' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sentence_transformers import SentenceTransformer \n",
    "embedding_model = SentenceTransformer(\"multi-qa-distilbert-cos-v1\")\n",
    "\n",
    "ultralytics_embeddings = []\n",
    "for d in tqdm(ultralytics_chunks):\n",
    "    v = embedding_model.encode(d[\"chunk\"])\n",
    "    ultralytics_embeddings.append(v)\n",
    "ultralytics_embeddings = np.array(ultralytics_embeddings)\n",
    "ultralytics_vindex = VectorSearch()\n",
    "ultralytics_vindex.fit(ultralytics_embeddings, ultralytics_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cd35ab8-62c3-4cc9-a848-58bcff76882c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06529615, -0.00864838,  0.02854672, ...,  0.01650504,\n",
       "        -0.0168132 ,  0.00734985],\n",
       "       [ 0.00462492,  0.01561103,  0.01850178, ..., -0.02181796,\n",
       "        -0.01262571,  0.02487436],\n",
       "       [ 0.01004778,  0.00531184,  0.03500869, ..., -0.054735  ,\n",
       "        -0.00501539,  0.03030851],\n",
       "       ...,\n",
       "       [ 0.05790858, -0.00230234,  0.07303187, ...,  0.00950586,\n",
       "         0.05140124, -0.02357837],\n",
       "       [ 0.05161763, -0.0034292 ,  0.08691493, ...,  0.0292849 ,\n",
       "         0.0410197 , -0.02345093],\n",
       "       [ 0.09797049, -0.02929091,  0.05815957, ...,  0.02632209,\n",
       "         0.01998046, -0.01946975]], shape=(3382, 768), dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ultralytics_embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bc41150-70cd-4cfe-ac98-b0ee05b55248",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"ultralytics_embeddings.txt\",ultralytics_embeddings)g_model.encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e24625e-d634-4134-8ca1-800040282acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import VectorSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4646987f-5997-419a-9dde-5264dcd2c89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x1e65b92e4d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ultralytics_vindex = VectorSearch()\n",
    "ultralytics_vindex.fit(ultralytics_embeddings, ultralytics_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bd33de0d-44d5-4e67-bd5b-2641cd9769d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Search Results:\n",
      "Number of results: 10\n",
      "First result keys: ['start', 'chunk', 'comments', 'description', 'keywords', 'filename']\n",
      "Question: ](https://github.com/THU-MIG/yoloe).\n",
      "\n",
      "## FAQ\n",
      "\n",
      "### How does YOLOE differ from YOLO-World?\n",
      "\n",
      "While both YOLOE and [YOLO-World](yolo-world.md) enable open-vocabulary detection, YOLOE offers several advantages. YOLOE achieves +3.5 AP higher accuracy on LVIS while using 3√ó less training resources and running 1.4√ó faster than YOLO-Worldv2. YOLOE also supports three prompting modes (text, visual, and internal vocabulary), whereas YOLO-World primarily focuses on text prompts. Additionally, YOLOE includes built-in [instance segmentation](https://www.ultralytics.com/blog/what-is-instance-segmentation-a-quick-guide) capabilities, providing pixel-precise masks for detected objects without additional overhead.\n",
      "\n",
      "### Can I use YOLOE as a regular YOLO model?\n",
      "\n",
      "Yes, YOLOE can function exactly like a standard YOLO model with no performance penalty. When used in closed-set mode (without prompts), YOLOE's open-vocabulary modules are re-parameterized into the standard detection head, resulting in identical speed and accuracy to equivalent YOLO11 models. This makes YOLOE extremely versatile‚Äîyou can use it as a traditional detector for maximum speed and then switch to open-vocabulary mode only when needed.\n",
      "\n",
      "### What types of prompts can I use with YOLOE?\n",
      "\n",
      "YOLOE supports three types of prompts:\n",
      "\n",
      "1. **Text prompts**: Specify object classes using natural language (e.g., \"person\", \"traffic light\", \"bird scooter\")\n",
      "2. **Visual prompts**: Provide reference images of objects you want to detect\n",
      "3. **Internal vocabulary**: Use YOLOE's built-in vocabulary of 1200+ categories without external prompts\n",
      "\n",
      "This flexibility allows you to adapt YOLOE to various scenarios without retraining the model, making it particularly useful for dynamic environments where detection requirements change frequently.\n",
      "\n",
      "### How does YOLOE handle instance segmentation?\n",
      "\n",
      "YOLOE integrates instance segmentation directly into its architecture by extending the detection head with a mask prediction branch. This approach is similar to\n",
      "--------------------------------------------------\n",
      "Question: if)\n",
      "\n",
      "## Overview\n",
      "\n",
      "YOLO-World tackles the challenges faced by traditional Open-Vocabulary detection models, which often rely on cumbersome [Transformer](https://www.ultralytics.com/glossary/transformer) models requiring extensive computational resources. These models' dependence on pre-defined object categories also restricts their utility in dynamic scenarios. YOLO-World revitalizes the YOLOv8 framework with open-vocabulary detection capabilities, employing vision-[language modeling](https://www.ultralytics.com/glossary/language-modeling) and pre-training on expansive datasets to excel at identifying a broad array of objects in zero-shot scenarios with unmatched efficiency.\n",
      "\n",
      "## Key Features\n",
      "\n",
      "1. **Real-time Solution:** Harnessing the computational speed of CNNs, YOLO-World delivers a swift open-vocabulary detection solution, catering to industries in need of immediate results.\n",
      "\n",
      "2. **Efficiency and Performance:** YOLO-World slashes computational and resource requirements without sacrificing performance, offering a robust alternative to models like SAM but at a fraction of the computational cost, enabling real-time applications.\n",
      "\n",
      "3. **Inference with Offline Vocabulary:** YOLO-World introduces a \"prompt-then-detect\" strategy, employing an offline vocabulary to enhance efficiency further. This approach enables the use of custom prompts computed apriori, including captions or categories, to be encoded and stored as offline vocabulary embeddings, streamlining the detection process.\n",
      "\n",
      "4. **Powered by YOLOv8:** Built upon [Ultralytics YOLOv8](yolov8.md), YOLO-World leverages the latest advancements in real-time object detection to facilitate open-vocabulary detection with unparalleled accuracy and speed.\n",
      "\n",
      "5. **Benchmark Excellence:** YOLO-World outperforms existing open-vocabulary detectors, including MDETR and GLIP series, in terms of speed and efficiency on standard benchmarks, showcasing YOLOv8's superior capability on a single NVIDIA V100 GPU.\n",
      "\n",
      "6. **Versatile Applications\n",
      "--------------------------------------------------\n",
      "Question: \n",
      "  <br>\n",
      "  <strong>Watch:</strong> How to use YOLOE with Ultralytics Python package: Open Vocabulary & Real-Time Seeing Anything üöÄ\n",
      "</p>\n",
      "\n",
      "Compared to earlier YOLO models, YOLOE significantly boosts efficiency and accuracy. It improves by **+3.5 AP** over YOLO-Worldv2 on LVIS while using just a third of the training resources and achieving 1.4√ó faster inference speeds. Fine-tuned on COCO, YOLOE-v8-large surpasses YOLOv8-L by **0.1 mAP**, using nearly **4√ó less training time**. This demonstrates YOLOE's exceptional balance of accuracy, efficiency, and versatility. The sections below explore YOLOE's architecture, benchmark comparisons, and integration with the [Ultralytics](https://www.ultralytics.com/) framework.\n",
      "\n",
      "## Architecture Overview\n",
      "\n",
      "<p align=\"center\">\n",
      "  <img src=\"https://github.com/THU-MIG/yoloe/raw/main/figures/pipeline.svg\" alt=\"YOLOE Architecture\" width=90%>\n",
      "</p>\n",
      "\n",
      "YOLOE retains the standard YOLO structure‚Äîa convolutional **backbone** (e.g., CSP-Darknet) for feature extraction, a **neck** (e.g., PAN-FPN) for multi-scale fusion, and an **anchor-free, decoupled** detection **head** (as in YOLOv8/YOLO11) predicting objectness, classes, and boxes independently. YOLOE introduces three novel modules enabling open-vocabulary detection:\n",
      "\n",
      "- **Re-parameterizable Region-Text Alignment (RepRTA)**: Supports **text-prompted detection** by refining text [embeddings](https://www.ultralytics.com/glossary/embeddings) (e.g., from CLIP) via a small auxiliary network. At inference, this network is folded into the main model, ensuring zero overhead. YOLOE thus detects arbitrary text-labeled objects (e.g., unseen \"traffic light\") without runtime penalties.\n",
      "\n",
      "- **Semantic-Activated Visual Prompt Encoder (SAVPE)**: Enables **visual-prompted detection** via a lightweight embedding branch. Given a reference image, SAVPE encodes semantic and activation features, conditioning the model to detect visually similar objects‚Äîa one-shot detection capability useful for logos or specific parts.\n",
      "\n",
      "- \n",
      "--------------------------------------------------\n",
      "Question: trating a **\"no free lunch trade-off\"** design.\n",
      "\n",
      "For zero-shot and transfer tasks, YOLOE excels: on LVIS, YOLOE-small improves over YOLO-Worldv2 by **+3.5 AP** using **3√ó less training resources**. Fine-tuning YOLOE-L from LVIS to COCO also required **4√ó less training time** than YOLOv8-L, underscoring its efficiency and adaptability. YOLOE further maintains YOLO's hallmark speed, achieving **300+ FPS** on a T4 GPU and **~64 FPS** on iPhone 12 via CoreML, ideal for edge and mobile deployments.\n",
      "\n",
      "!!! note\n",
      "\n",
      "    **Benchmark conditions:** YOLOE results are from models pre-trained on Objects365, GoldG, and LVIS, then fine-tuned or evaluated on COCO. YOLOE's slight mAP advantage over YOLOv8 comes from extensive pre-training. Without this open-vocab training, YOLOE matches similar-sized YOLO models, affirming its SOTA accuracy and open-world flexibility without performance penalties.\n",
      "\n",
      "## Comparison with Previous Models\n",
      "\n",
      "YOLOE introduces notable advancements over prior YOLO models and open-vocabulary detectors:\n",
      "\n",
      "- **YOLOE vs YOLOv5:**\n",
      "  [YOLOv5](yolov5.md) offered good speed-accuracy balance but required retraining for new classes and used anchor-based heads. In contrast, YOLOE is **anchor-free** and dynamically detects new classes. YOLOE, building on YOLOv8's improvements, achieves higher accuracy (52.6% vs. YOLOv5's ~50% mAP on COCO) and integrates instance segmentation, unlike YOLOv5.\n",
      "\n",
      "- **YOLOE vs YOLOv8:**\n",
      "  YOLOE extends [YOLOv8](yolov8.md)'s redesigned architecture, achieving similar or superior accuracy (**52.6% mAP with ~26M parameters** vs. YOLOv8-L's **52.9% with ~44M parameters**). It significantly reduces training time due to stronger pre-training. The key advancement is YOLOE's **open-world capability**, detecting unseen objects (e.g., \"**bird scooter**\" or \"**peace symbol**\") via prompts, unlike YOLOv8's closed-set design.\n",
      "\n",
      "- **YOLOE vs YOLO11:**\n",
      "  [YOLO11](yolo11.md) improves upon YOLOv8 with enhanced efficiency and fewer parameters (~22% reduction). YOLOE in\n",
      "--------------------------------------------------\n",
      "Question: information on YOLO architecture, features, and usage, please refer to our [GitHub repository](https://github.com/ultralytics/ultralytics) and [documentation](https://docs.ultralytics.com/).\n",
      "\n",
      "If you use YOLOv5 or YOLOv5u in your research, please cite the Ultralytics YOLOv5 repository as follows:\n",
      "\n",
      "!!! quote \"\"\n",
      "\n",
      "    === \"BibTeX\"\n",
      "\n",
      "        ```bibtex\n",
      "        @software{yolov5,\n",
      "          title = {Ultralytics YOLOv5},\n",
      "          author = {Glenn Jocher},\n",
      "          year = {2020},\n",
      "          version = {7.0},\n",
      "          license = {AGPL-3.0},\n",
      "          url = {https://github.com/ultralytics/yolov5},\n",
      "          doi = {10.5281/zenodo.3908559},\n",
      "          orcid = {0000-0001-5950-6979}\n",
      "        }\n",
      "        ```\n",
      "\n",
      "Please note that YOLOv5 models are provided under [AGPL-3.0](https://github.com/ultralytics/ultralytics/blob/main/LICENSE) and [Enterprise](https://www.ultralytics.com/license) licenses.\n",
      "\n",
      "## FAQ\n",
      "\n",
      "### What is Ultralytics YOLOv5u and how does it differ from YOLOv5?\n",
      "\n",
      "Ultralytics YOLOv5u is an advanced version of YOLOv5, integrating the anchor-free, objectness-free split head that enhances the [accuracy](https://www.ultralytics.com/glossary/accuracy)-speed tradeoff for real-time object detection tasks. Unlike the traditional YOLOv5, YOLOv5u adopts an anchor-free detection mechanism, making it more flexible and adaptive in diverse scenarios. For more detailed information on its features, you can refer to the [YOLOv5 Overview](#overview).\n",
      "\n",
      "### How does the anchor-free Ultralytics head improve object detection performance in YOLOv5u?\n",
      "\n",
      "The anchor-free Ultralytics head in YOLOv5u improves object detection performance by eliminating the dependency on predefined anchor boxes. This results in a more flexible and adaptive detection mechanism that can handle various object sizes and shapes with greater efficiency. This enhancement directly contributes to a balanced tradeoff between accuracy and speed, making YOLOv5u suitable for real-time applications. Learn more about its architecture in the [Key \n",
      "--------------------------------------------------\n",
      "Question: r; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n",
      "    allowfullscreen>\n",
      "  </iframe>\n",
      "  <br>\n",
      "  <strong>Watch:</strong> Ultralytics YOLO11 Guides Overview\n",
      "</p>\n",
      "\n",
      "## Guides\n",
      "\n",
      "Here's a compilation of in-depth guides to help you master different aspects of Ultralytics YOLO.\n",
      "\n",
      "- [YOLO Common Issues](yolo-common-issues.md) ‚≠ê RECOMMENDED: Practical solutions and troubleshooting tips to the most frequently encountered issues when working with Ultralytics YOLO models.\n",
      "- [YOLO Performance Metrics](yolo-performance-metrics.md) ‚≠ê ESSENTIAL: Understand the key metrics like mAP, IoU, and [F1 score](https://www.ultralytics.com/glossary/f1-score) used to evaluate the performance of your YOLO models. Includes practical examples and tips on how to improve detection accuracy and speed.\n",
      "- [YOLO Thread-Safe Inference](yolo-thread-safe-inference.md) üöÄ NEW: Guidelines for performing inference with YOLO models in a thread-safe manner. Learn the importance of thread safety and best practices to prevent race conditions and ensure consistent predictions.\n",
      "- [YOLO Data Augmentation](yolo-data-augmentation.md) üöÄ NEW: Master the complete range of data augmentation techniques in YOLO, from basic transformations to advanced strategies for improving model robustness and performance.\n",
      "- [Model Deployment Options](model-deployment-options.md): Overview of YOLO [model deployment](https://www.ultralytics.com/glossary/model-deployment) formats like ONNX, OpenVINO, and TensorRT, with pros and cons for each to inform your deployment strategy.\n",
      "- [Model YAML Configuration Guide](model-yaml-config.md) üöÄ NEW: A comprehensive deep dive into Ultralytics' model architecture definitions. Explore the YAML format, understand the module resolution system, and learn how to integrate custom modules seamlessly.\n",
      "- [K-Fold Cross Validation](kfold-cross-validation.md) üöÄ NEW: Learn how to improve model generalization using K-Fold cross-validation technique.\n",
      "- [Hyperparameter Tuning](hyperpara\n",
      "--------------------------------------------------\n",
      "Question: # YOLOE: Real-Time Seeing Anything\n",
      "\n",
      "## Introduction\n",
      "\n",
      "![YOLOE Prompting Options](https://raw.githubusercontent.com/THU-MIG/yoloe/main/figures/visualization.svg)\n",
      "\n",
      "[YOLOE (Real-Time Seeing Anything)](https://arxiv.org/html/2503.07465v1) is a new advancement in zero-shot, promptable YOLO models, designed for **open-vocabulary** detection and segmentation. Unlike previous YOLO models limited to fixed categories, YOLOE uses text, image, or internal vocabulary prompts, enabling real-time detection of any object class. Built upon YOLOv10 and inspired by [YOLO-World](yolo-world.md), YOLOE achieves **state-of-the-art zero-shot performance** with minimal impact on speed and accuracy.\n",
      "\n",
      "<p align=\"center\">\n",
      "  <br>\n",
      "  <iframe loading=\"lazy\" width=\"720\" height=\"405\" src=\"https://www.youtube.com/embed/HMOoM2NwFIQ\"\n",
      "    title=\"YouTube video player\" frameborder=\"0\"\n",
      "    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n",
      "    allowfullscreen>\n",
      "  </iframe>\n",
      "  <br>\n",
      "  <strong>Watch:</strong> How to use YOLOE with Ultralytics Python package: Open Vocabulary & Real-Time Seeing Anything üöÄ\n",
      "</p>\n",
      "\n",
      "Compared to earlier YOLO models, YOLOE significantly boosts efficiency and accuracy. It improves by **+3.5 AP** over YOLO-Worldv2 on LVIS while using just a third of the training resources and achieving 1.4√ó faster inference speeds. Fine-tuned on COCO, YOLOE-v8-large surpasses YOLOv8-L by **0.1 mAP**, using nearly **4√ó less training time**. This demonstrates YOLOE's exceptional balance of accuracy, efficiency, and versatility. The sections below explore YOLOE's architecture, benchmark comparisons, and integration with the [Ultralytics](https://www.ultralytics.com/) framework.\n",
      "\n",
      "## Architecture Overview\n",
      "\n",
      "<p align=\"center\">\n",
      "  <img src=\"https://github.com/THU-MIG/yoloe/raw/main/figures/pipeline.svg\" alt=\"YOLOE Architecture\" width=90%>\n",
      "</p>\n",
      "\n",
      "YOLOE retains the standard YOLO structure‚Äîa convolutional **backbone** (e.g., CSP-Darknet) for feature extraction, a \n",
      "--------------------------------------------------\n",
      "Question: # Comprehensive Tutorials to Ultralytics YOLO\n",
      "\n",
      "Welcome to the Ultralytics' YOLO üöÄ Guides! Our comprehensive tutorials cover various aspects of the YOLO [object detection](https://www.ultralytics.com/glossary/object-detection) model, ranging from training and prediction to deployment. Built on [PyTorch](https://www.ultralytics.com/glossary/pytorch), YOLO stands out for its exceptional speed and [accuracy](https://www.ultralytics.com/glossary/accuracy) in real-time object detection tasks.\n",
      "\n",
      "Whether you're a beginner or an expert in [deep learning](https://www.ultralytics.com/glossary/deep-learning-dl), our tutorials offer valuable insights into the implementation and optimization of YOLO for your [computer vision](https://www.ultralytics.com/glossary/computer-vision-cv) projects. Let's dive in!\n",
      "\n",
      "<p align=\"center\">\n",
      "  <br>\n",
      "  <iframe loading=\"lazy\" width=\"720\" height=\"405\" src=\"https://www.youtube.com/embed/96NkhsV-W1U\"\n",
      "    title=\"YouTube video player\" frameborder=\"0\"\n",
      "    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n",
      "    allowfullscreen>\n",
      "  </iframe>\n",
      "  <br>\n",
      "  <strong>Watch:</strong> Ultralytics YOLO11 Guides Overview\n",
      "</p>\n",
      "\n",
      "## Guides\n",
      "\n",
      "Here's a compilation of in-depth guides to help you master different aspects of Ultralytics YOLO.\n",
      "\n",
      "- [YOLO Common Issues](yolo-common-issues.md) ‚≠ê RECOMMENDED: Practical solutions and troubleshooting tips to the most frequently encountered issues when working with Ultralytics YOLO models.\n",
      "- [YOLO Performance Metrics](yolo-performance-metrics.md) ‚≠ê ESSENTIAL: Understand the key metrics like mAP, IoU, and [F1 score](https://www.ultralytics.com/glossary/f1-score) used to evaluate the performance of your YOLO models. Includes practical examples and tips on how to improve detection accuracy and speed.\n",
      "- [YOLO Thread-Safe Inference](yolo-thread-safe-inference.md) üöÄ NEW: Guidelines for performing inference with YOLO models in a thread-safe manner. Learn the importance of thread safety and \n",
      "--------------------------------------------------\n",
      "Question:  models. We focus on advancing the technology and making it easier to use, rather than producing static documentation. For the most up-to-date information on YOLO architecture, features, and usage, please refer to our [GitHub repository](https://github.com/ultralytics/ultralytics) and [documentation](https://docs.ultralytics.com/).\n",
      "\n",
      "If you use the YOLOv8 model or any other software from this repository in your work, please cite it using the following format:\n",
      "\n",
      "!!! quote \"\"\n",
      "\n",
      "    === \"BibTeX\"\n",
      "\n",
      "        ```bibtex\n",
      "        @software{yolov8_ultralytics,\n",
      "          author = {Glenn Jocher and Ayush Chaurasia and Jing Qiu},\n",
      "          title = {Ultralytics YOLOv8},\n",
      "          version = {8.0.0},\n",
      "          year = {2023},\n",
      "          url = {https://github.com/ultralytics/ultralytics},\n",
      "          orcid = {0000-0001-5950-6979, 0000-0002-7603-6750, 0000-0003-3783-7069},\n",
      "          license = {AGPL-3.0}\n",
      "        }\n",
      "        ```\n",
      "\n",
      "Please note that the DOI is pending and will be added to the citation once it is available. YOLOv8 models are provided under [AGPL-3.0](https://github.com/ultralytics/ultralytics/blob/main/LICENSE) and [Enterprise](https://www.ultralytics.com/license) licenses.\n",
      "\n",
      "## FAQ\n",
      "\n",
      "### What is YOLOv8 and how does it differ from previous YOLO versions?\n",
      "\n",
      "YOLOv8 is designed to improve real-time object detection performance with advanced features. Unlike earlier versions, YOLOv8 incorporates an **anchor-free split Ultralytics head**, state-of-the-art [backbone](https://www.ultralytics.com/glossary/backbone) and neck architectures, and offers optimized [accuracy](https://www.ultralytics.com/glossary/accuracy)-speed tradeoff, making it ideal for diverse applications. For more details, check the [Overview](#overview) and [Key Features](#key-features-of-yolov8) sections.\n",
      "\n",
      "### How can I use YOLOv8 for different computer vision tasks?\n",
      "\n",
      "YOLOv8 supports a wide range of computer vision tasks, including object detection, instance segmentation, pose/keypoints detection, oriented object detection, \n",
      "--------------------------------------------------\n",
      "Question: bulary detectors:\n",
      "\n",
      "- **YOLOE vs YOLOv5:**\n",
      "  [YOLOv5](yolov5.md) offered good speed-accuracy balance but required retraining for new classes and used anchor-based heads. In contrast, YOLOE is **anchor-free** and dynamically detects new classes. YOLOE, building on YOLOv8's improvements, achieves higher accuracy (52.6% vs. YOLOv5's ~50% mAP on COCO) and integrates instance segmentation, unlike YOLOv5.\n",
      "\n",
      "- **YOLOE vs YOLOv8:**\n",
      "  YOLOE extends [YOLOv8](yolov8.md)'s redesigned architecture, achieving similar or superior accuracy (**52.6% mAP with ~26M parameters** vs. YOLOv8-L's **52.9% with ~44M parameters**). It significantly reduces training time due to stronger pre-training. The key advancement is YOLOE's **open-world capability**, detecting unseen objects (e.g., \"**bird scooter**\" or \"**peace symbol**\") via prompts, unlike YOLOv8's closed-set design.\n",
      "\n",
      "- **YOLOE vs YOLO11:**\n",
      "  [YOLO11](yolo11.md) improves upon YOLOv8 with enhanced efficiency and fewer parameters (~22% reduction). YOLOE inherits these gains directly, matching YOLO11's inference speed and parameter count (~26M parameters), while adding **open-vocabulary detection and segmentation**. In closed-set scenarios, YOLOE is equivalent to YOLO11, but crucially adds adaptability to detect unseen classes, achieving **YOLO11 + open-world capability** without compromising speed.\n",
      "\n",
      "- **YOLOE vs previous open-vocabulary detectors:**\n",
      "  Earlier open-vocab models (GLIP, OWL-ViT, [YOLO-World](yolo-world.md)) relied heavily on vision-language [transformers](https://www.ultralytics.com/glossary/transformer), leading to slow inference. YOLOE surpasses these in zero-shot accuracy (e.g., **+3.5 AP vs. YOLO-Worldv2**) while running **1.4√ó faster** with significantly lower training resources. Compared to transformer-based approaches (e.g., GLIP), YOLOE offers orders-of-magnitude faster inference, effectively bridging the accuracy-efficiency gap in open-set detection.\n",
      "\n",
      "In summary, YOLOE maintains YOLO's renowned speed and efficienc\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "vector_query = \" Describe YOLO\" \n",
    "q = embedding_model.encode(vector_query)\n",
    "vector_results = ultralytics_vindex.search(q, num_results = 10)\n",
    "print(\"Vector Search Results:\")\n",
    "print(f\"Number of results: {len(vector_results)}\")\n",
    "if vector_results:\n",
    "    print(f\"First result keys: {list(vector_results[0].keys())}\")\n",
    "for result in vector_results:\n",
    "    print(f\"Question: {result['chunk']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa76de4-b40d-4ec8-afca-0878a51a3a1b",
   "metadata": {},
   "source": [
    "# Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "425c77b4-d30a-433e-80d3-67c088dba02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How can i train a YOLO model\"\n",
    "text_results = index.search(query, num_results = 5)\n",
    "q = embedding_model.encode(query)\n",
    "vector_results = ultralytics_vindex.search(q, num_results = 5)\n",
    "final_results = text_results + vector_results \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "670a5144-ebee-4568-9404-42fae3c88fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Search Results:\n",
      "\n",
      "1. [Source: ultralytics-main/docs/en/reference/models/yolo/world/train.md]\n",
      "   Question: How can i train a YOLO model\n",
      "   Answer: # Reference for `ultralytics/models/yolo/world/train.py`\n",
      "\n",
      "!!! note\n",
      "\n",
      "    This file is available at [https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/world/train.py](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/world/train.py). If you spot a problem please help fix it by [contributing](https://docs.ultralytics.com/help/contributing/) a [Pull Request](https://github.com/ultralytics/ultralytics/edit/main/ultralytics/models/yolo/world/train....\n",
      "\n",
      "2. [Source: ultralytics-main/docs/en/reference/models/yolo/segment/train.md]\n",
      "   Question: How can i train a YOLO model\n",
      "   Answer: # Reference for `ultralytics/models/yolo/segment/train.py`\n",
      "\n",
      "!!! note\n",
      "\n",
      "    This file is available at [https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/segment/train.py](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/segment/train.py). If you spot a problem please help fix it by [contributing](https://docs.ultralytics.com/help/contributing/) a [Pull Request](https://github.com/ultralytics/ultralytics/edit/main/ultralytics/models/yolo/segmen...\n",
      "\n",
      "3. [Source: ultralytics-main/docs/en/reference/models/yolo/classify/train.md]\n",
      "   Question: How can i train a YOLO model\n",
      "   Answer: # Reference for `ultralytics/models/yolo/classify/train.py`\n",
      "\n",
      "!!! note\n",
      "\n",
      "    This file is available at [https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/classify/train.py](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/classify/train.py). If you spot a problem please help fix it by [contributing](https://docs.ultralytics.com/help/contributing/) a [Pull Request](https://github.com/ultralytics/ultralytics/edit/main/ultralytics/models/yolo/cla...\n",
      "\n",
      "4. [Source: ultralytics-main/docs/en/modes/train.md]\n",
      "   Question: How can i train a YOLO model\n",
      "   Answer: ns # replace with 'runs' directory\n",
      "        ```\n",
      "\n",
      "This will load TensorBoard and direct it to the directory where your training logs are saved.\n",
      "\n",
      "After setting up your logger, you can then proceed with your model training. All training metrics will be automatically logged in your chosen platform, and you can access these logs to monitor your model's performance over time, compare different models, and identify areas for improvement.\n",
      "\n",
      "## FAQ\n",
      "\n",
      "### How do I train an [object detection](https://www.ultr...\n",
      "\n",
      "5. [Source: ultralytics-main/docs/en/modes/train.md]\n",
      "   Question: How can i train a YOLO model\n",
      "   Answer: ain__\":` block before your training code to resolve it.\n",
      "\n",
      "!!! example \"Single-GPU and CPU Training Example\"\n",
      "\n",
      "    Device is determined automatically. If a GPU is available then it will be used (default CUDA device 0), otherwise training will start on CPU.\n",
      "\n",
      "    === \"Python\"\n",
      "\n",
      "        ```python\n",
      "        from ultralytics import YOLO\n",
      "\n",
      "        # Load a model\n",
      "        model = YOLO(\"yolo11n.yaml\")  # build a new model from YAML\n",
      "        model = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for t...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_results.sort(key=lambda x: x.get('score', 0), reverse=True)\n",
    "\n",
    "# Limit to top N results\n",
    "num_results = 5\n",
    "final_results = final_results[:num_results]\n",
    "\n",
    "print(\"Hybrid Search Results:\\n\")\n",
    "for i, result in enumerate(final_results, 1):\n",
    "    print(f\"{i}. [Source: {result.get('filename', 'unknown')}]\")\n",
    "    print(f\"   Question: {query}\")\n",
    "    # Prefer 'description', otherwise show snippet from 'chunk'\n",
    "    answer = result.get('chunk', '')\n",
    "    print(f\"   Answer: {answer[:500]}...\")   \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b99e8acb-f4c6-4f0d-8f0f-66886a58e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_search(query):\n",
    "    return index.search(query, num_results=5)\n",
    "\n",
    "def vector_search(query):\n",
    "    q = embedding_model.encode(query)\n",
    "    return ultralytics_vindex.search(q, num_results=5)\n",
    "\n",
    "def hybrid_search(query):\n",
    "    text_results = text_search(query)\n",
    "    vector_results = vector_search(query)\n",
    "    \n",
    "    # Combine and deduplicate results\n",
    "    seen_ids = set()\n",
    "    combined_results = []\n",
    "\n",
    "    for result in text_results + vector_results:\n",
    "        if result['filename'] not in seen_ids:\n",
    "            seen_ids.add(result['filename'])\n",
    "            combined_results.append(result)\n",
    "    \n",
    "    return combined_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9249795a-af7d-4694-b74a-496adeefa06b",
   "metadata": {},
   "source": [
    "# Day 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "82f1205c-a2b4-499d-85fa-6aeb62079c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv \n",
    "import os \n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8ee749da-ecd4-4a3b-8c5f-354bb0099fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"That's fantastic! It's great you've discovered a course that interests you.\\n\\nWhether you can join now really depends on a few key things about the specific course:\\n\\n1.  **What is the name of the course?** (This is the most important piece of information!)\\n2.  **Where are you seeing it/where is it offered?** (e.g., Coursera, Udemy, edX, a university website, a specific learning platform, a company training portal?)\\n3.  **Is it a self-paced course, or does it have specific start/end dates and live sessions (a 'cohort-based' course)?**\\n\\n**Here's why these details matter:**\\n\\n*   **Self-paced courses:** For many online courses (especially on platforms like Coursera, Udemy, edX, etc.), if they are self-paced, you can often **enroll anytime and start immediately.**\\n*   **Cohort-based courses:** These usually have specific enrollment periods, application deadlines, and fixed start dates. If it's a course with a structured schedule, you might need to wait for the next cohort, or the current enrollment period might have closed.\\n*   **University/College Courses:** These typically follow academic calendars with strict registration deadlines.\\n\\n**To give you the best answer, please tell me:**\\n\\n*   **What is the name of the course?**\\n*   **Where did you find it/where is it hosted?**\\n\\nOnce I have that information, I can try to help you find out if you can join right away!\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI \n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "user_prompt = \"I just discovered the course, can I join now?\"\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    messages= chat_messages,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "743e2e60-ee5b-445e-b12b-93829115d342",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_search_tool = {\n",
    "    \"name\": \"text_search\",\n",
    "    \"description\": \"Search the Ultralytics database\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Search query text to look up in the Ultralyics documentation.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75de38ac-c8e3-487c-b405-900f13b2c116",
   "metadata": {},
   "source": [
    "Trying out different system prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3bc31c0d-14cc-4226-9b63-26adcc1e7b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompts = [\"You are a helpful AI assistant tasked with answering questions based on the Ultralytics YOLO documentation\",\"You are a helpful AI assistant for a course on computer vision, specializing in YOLO models and the Ultralytics ecosystem (e.g., YOLOv8). Use the search tool to find relevant information from the course materials or the latest Ultralytics documentation before answering questions. Provide accurate, concise, and clear responses, including code examples, step-by-step instructions, or explanations as appropriate. Assume the user has a basic understanding of Python and machine learning concepts unless otherwise specified. If the search returns specific, relevant information, use it to inform your answer. If the search does not yield relevant results, inform the user and provide general guidance based on best practices for YOLO model training and usage. If a question is ambiguous, ask for clarification to ensure the response aligns with the course context.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7cb360f8-3579-4bbf-b302-0ab139ac1877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=None args={'query': 'train YOLO model'} name='text_search'\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "\n",
    "client = genai.Client(api_key= api_key)\n",
    "tools = types.Tool(function_declarations=[text_search_tool])\n",
    "config = types.GenerateContentConfig(tools=[tools],system_instruction=system_prompts[1])\n",
    "\n",
    "\n",
    "contents = [\n",
    "    types.Content(\n",
    "        role=\"user\", parts=[types.Part(text=\"How can I train a YOLO model?\")]\n",
    "    )\n",
    "]\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=contents,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "\n",
    "print(response.candidates[0].content.parts[0].function_call)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e8aa76c8-6d93-4398-99f1-b5121d7ea289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function execution result: [{'start': 0, 'chunk': '# Reference for `ultralytics/models/yolo/world/train.py`\\n\\n!!! note\\n\\n    This file is available at [https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/world/train.py](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/world/train.py). If you spot a problem please help fix it by [contributing](https://docs.ultralytics.com/help/contributing/) a [Pull Request](https://github.com/ultralytics/ultralytics/edit/main/ultralytics/models/yolo/world/train.py) üõ†Ô∏è. Thank you üôè!\\n\\n<br>\\n\\n## ::: ultralytics.models.yolo.world.train.WorldTrainer\\n\\n<br><br><hr><br>\\n\\n## ::: ultralytics.models.yolo.world.train.on_pretrain_routine_end\\n\\n<br><br>', 'description': 'Learn how to train a World Model with Ultralytics YOLO using advanced techniques and customizable options for optimal performance.', 'keywords': 'Ultralytics, YOLO, World Model, training, deep learning, computer vision, AI, machine learning, tutorial', 'filename': 'ultralytics-main/docs/en/reference/models/yolo/world/train.md'}, {'start': 0, 'chunk': '# Reference for `ultralytics/models/yolo/classify/train.py`\\n\\n!!! note\\n\\n    This file is available at [https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/classify/train.py](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/classify/train.py). If you spot a problem please help fix it by [contributing](https://docs.ultralytics.com/help/contributing/) a [Pull Request](https://github.com/ultralytics/ultralytics/edit/main/ultralytics/models/yolo/classify/train.py) üõ†Ô∏è. Thank you üôè!\\n\\n<br>\\n\\n## ::: ultralytics.models.yolo.classify.train.ClassificationTrainer\\n\\n<br><br>', 'description': 'Explore the train.py module in Ultralytics YOLO for efficient classification model training. Learn more with examples and detailed code documentation.', 'keywords': 'YOLO, Ultralytics, classification, training, machine learning, deep learning, PyTorch, train.py', 'filename': 'ultralytics-main/docs/en/reference/models/yolo/classify/train.md'}, {'start': 0, 'chunk': '# Reference for `ultralytics/models/yolo/segment/train.py`\\n\\n!!! note\\n\\n    This file is available at [https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/segment/train.py](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/segment/train.py). If you spot a problem please help fix it by [contributing](https://docs.ultralytics.com/help/contributing/) a [Pull Request](https://github.com/ultralytics/ultralytics/edit/main/ultralytics/models/yolo/segment/train.py) üõ†Ô∏è. Thank you üôè!\\n\\n<br>\\n\\n## ::: ultralytics.models.yolo.segment.train.SegmentationTrainer\\n\\n<br><br>', 'description': 'Learn how to train YOLO models for segmentation tasks with Ultralytics. Explore the SegmentationTrainer class and its functionalities.', 'keywords': 'YOLO, segmentation, train, Ultralytics, SegmentationTrainer, Python, machine learning, deep learning, tutorials', 'filename': 'ultralytics-main/docs/en/reference/models/yolo/segment/train.md'}, {'start': 0, 'chunk': '# Reference for `ultralytics/models/yolo/model.py`\\n\\n!!! note\\n\\n    This file is available at [https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/model.py](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/model.py). If you spot a problem please help fix it by [contributing](https://docs.ultralytics.com/help/contributing/) a [Pull Request](https://github.com/ultralytics/ultralytics/edit/main/ultralytics/models/yolo/model.py) üõ†Ô∏è. Thank you üôè!\\n\\n<br>\\n\\n## ::: ultralytics.models.yolo.model.YOLO\\n\\n<br><br><hr><br>\\n\\n## ::: ultralytics.models.yolo.model.YOLOWorld\\n\\n<br><br><hr><br>\\n\\n## ::: ultralytics.models.yolo.model.YOLOE\\n\\n<br><br>', 'description': 'Explore the ultralytics.models.yolo.model module for YOLO object detection. Learn initialization, model mapping, and more.', 'keywords': 'YOLO, object detection, Ultralytics, YOLO model, machine learning, Python, model initialization', 'filename': 'ultralytics-main/docs/en/reference/models/yolo/model.md'}, {'start': 15000, 'chunk': 'ns # replace with \\'runs\\' directory\\n        ```\\n\\nThis will load TensorBoard and direct it to the directory where your training logs are saved.\\n\\nAfter setting up your logger, you can then proceed with your model training. All training metrics will be automatically logged in your chosen platform, and you can access these logs to monitor your model\\'s performance over time, compare different models, and identify areas for improvement.\\n\\n## FAQ\\n\\n### How do I train an [object detection](https://www.ultralytics.com/glossary/object-detection) model using Ultralytics YOLO11?\\n\\nTo train an object detection model using Ultralytics YOLO11, you can either use the Python API or the CLI. Below is an example for both:\\n\\n!!! example \"Single-GPU and CPU Training Example\"\\n\\n    === \"Python\"\\n\\n        ```python\\n        from ultralytics import YOLO\\n\\n        # Load a model\\n        model = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\\n\\n        # Train the model\\n        results = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)\\n        ```\\n\\n    === \"CLI\"\\n\\n        ```bash\\n        yolo detect train data=coco8.yaml model=yolo11n.pt epochs=100 imgsz=640\\n        ```\\n\\nFor more details, refer to the [Train Settings](#train-settings) section.\\n\\n### What are the key features of Ultralytics YOLO11\\'s Train mode?\\n\\nThe key features of Ultralytics YOLO11\\'s Train mode include:\\n\\n- **Automatic Dataset Download:** Automatically downloads standard datasets like COCO, VOC, and ImageNet.\\n- **Multi-GPU Support:** Scale training across multiple GPUs for faster processing.\\n- **Hyperparameter Configuration:** Customize hyperparameters through YAML files or CLI arguments.\\n- **Visualization and Monitoring:** Real-time tracking of training metrics for better insights.\\n\\nThese features make training efficient and customizable to your needs. For more details, see the [Key Features of Train Mode](#key-features-of-train-mode) section.\\n\\n### How do I resume training from an interrupted session in Ul', 'comments': True, 'description': 'Learn how to efficiently train object detection models using YOLO11 with comprehensive instructions on settings, augmentation, and hardware utilization.', 'keywords': 'Ultralytics, YOLO11, model training, deep learning, object detection, GPU training, dataset augmentation, hyperparameter tuning, model performance, apple silicon training', 'filename': 'ultralytics-main/docs/en/modes/train.md'}]\n"
     ]
    }
   ],
   "source": [
    "# Extract tool call details, it may not be in the first part.\n",
    "tool_call = response.candidates[0].content.parts[0].function_call\n",
    "\n",
    "if tool_call.name == \"text_search\":\n",
    "    result = text_search(**tool_call.args)\n",
    "    print(f\"Function execution result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d9030927-7ade-488d-bfff-c32433573cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To train a YOLO model using Ultralytics, you primarily need a dataset and can leverage either the Python API or the command-line interface (CLI). Here's a breakdown of the process:\n",
      "\n",
      "### Prerequisites\n",
      "\n",
      "1.  **Install Ultralytics:** Ensure you have the Ultralytics package installed. You can do this via pip:\n",
      "    ```bash\n",
      "    pip install ultralytics\n",
      "    ```\n",
      "2.  **Prepare your Dataset:** Your dataset should be in the YOLO format. This typically involves:\n",
      "    *   An image directory.\n",
      "    *   A corresponding labels directory with `.txt` files for each image, defining bounding box coordinates and class IDs.\n",
      "    *   A `.yaml` file that specifies the path to your data, class names, and the number of classes.\n",
      "\n",
      "### Training Steps\n",
      "\n",
      "**1. Using the Python API (Recommended for flexibility):**\n",
      "\n",
      "This is the most common and flexible way to train your YOLO model.\n",
      "\n",
      "```python\n",
      "from ultralytics import YOLO\n",
      "\n",
      "# 1. Load a pretrained YOLO model (recommended for faster convergence and better performance)\n",
      "# You can choose different model sizes, e.g., 'yolov8n.pt' (nano), 'yolov8s.pt' (small), 'yolov8m.pt' (medium), etc.\n",
      "model = YOLO(\"yolov8n.pt\")\n",
      "\n",
      "# 2. Train the model\n",
      "# 'data' points to your dataset's YAML file.\n",
      "# 'epochs' defines the number of training iterations.\n",
      "# 'imgsz' sets the input image size for training.\n",
      "results = model.train(data=\"path/to/your_dataset.yaml\", epochs=100, imgsz=640)\n",
      "\n",
      "# 3. (Optional) Evaluate the model on the validation set\n",
      "# results = model.val()\n",
      "\n",
      "# 4. (Optional) Export the model to a different format (e.g., ONNX, OpenVINO, TFLite)\n",
      "# success = model.export(format=\"onnx\")\n",
      "```\n",
      "\n",
      "**Key Parameters for `model.train()`:**\n",
      "\n",
      "*   `data`: Path to your dataset configuration YAML file.\n",
      "*   `epochs`: Number of full passes through the training dataset. More epochs can lead to better performance but also risk overfitting.\n",
      "*   `imgsz`: Input image size (e.g., 640 for 640x640 pixels).\n",
      "*   `batch`: Batch size, i.e., the number of images processed at once. Larger batches require more GPU memory.\n",
      "*   `name`: A name for your training run, which will create a directory in `runs/detect/` (or `runs/classify/`, `runs/segment/`) to store results.\n",
      "*   `device`: Specify GPU devices (e.g., `device=0` for the first GPU, `device='0,1'` for multiple GPUs, `device='cpu'` for CPU training).\n",
      "*   `patience`: Number of epochs to wait for no improvement in validation metrics before early stopping.\n",
      "*   `optimizer`: Optimizer to use (e.g., 'SGD', 'Adam', 'AdamW').\n",
      "*   `lr0`: Initial learning rate.\n",
      "\n",
      "**2. Using the Command-Line Interface (CLI):**\n",
      "\n",
      "The CLI is convenient for quick experiments or scripting.\n",
      "\n",
      "```bash\n",
      "yolo detect train data=path/to/your_dataset.yaml model=yolov8n.pt epochs=100 imgsz=640 batch=16 name=my_custom_yolo\n",
      "```\n",
      "\n",
      "**Explanation of CLI arguments:**\n",
      "\n",
      "*   `yolo`: Invokes the Ultralytics YOLO command-line tool.\n",
      "*   `detect train`: Specifies that you want to perform an object detection training task. Other modes include `segment train` for segmentation or `classify train` for classification.\n",
      "*   `data=path/to/your_dataset.yaml`: Same as the `data` parameter in the Python API.\n",
      "*   `model=yolov8n.pt`: Specifies the pretrained model to use.\n",
      "*   `epochs=100`: Same as the `epochs` parameter.\n",
      "*   `imgsz=640`: Same as the `imgsz` parameter.\n",
      "*   `batch=16`: Sets the batch size.\n",
      "*   `name=my_custom_yolo`: Sets the run name.\n",
      "\n",
      "### Monitoring Training\n",
      "\n",
      "Ultralytics YOLO automatically logs training metrics and saves them. You can monitor the training progress using tools like TensorBoard, which can be launched from your `runs/` directory:\n",
      "\n",
      "```bash\n",
      "tensorboard --logdir runs\n",
      "```\n",
      "\n",
      "This will display various graphs and metrics, helping you understand your model's performance over time.\n"
     ]
    }
   ],
   "source": [
    "function_response_part = types.Part.from_function_response(\n",
    "    name=tool_call.name,\n",
    "    response={\"result\": result},\n",
    ")\n",
    "# Append function call and result of the function execution to contents\n",
    "contents.append(response.candidates[0].content) # Append the content from the model's response.\n",
    "contents.append(types.Content(role=\"user\", parts=[function_response_part])) # Append the function response\n",
    "final_response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config=config,\n",
    "    contents=contents,\n",
    ")\n",
    "\n",
    "print(final_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f5ac99-22ab-4cd4-85b7-25b753a0ee97",
   "metadata": {},
   "source": [
    "# Pydantic AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "69f350a3-0908-4535-afa3-2f4fab73b115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2c153daf-813e-4db8-a5a0-2394c84aef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform a text-based search on the FAQ index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of up to 5 search results returned by the FAQ index.\n",
    "    \"\"\"\n",
    "    return index.search(query, num_results=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "66d30179-bf7a-4176-9bef-6c56a98f9e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.providers.google import GoogleProvider\n",
    "from pydantic_ai.models.google import GoogleModel\n",
    " \n",
    "provider = GoogleProvider(api_key=api_key)\n",
    "google_model = GoogleModel(\"gemini-2.5-flash\", provider=provider)\n",
    "\n",
    "# Initialize the Agent, passing the provider explicitly\n",
    "agent = Agent(\n",
    "    name=\"faq_agent\",\n",
    "    instructions=system_prompt[1],  \n",
    "    tools=[text_search],           \n",
    "    model=google_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a0521463-14ab-4120-905a-5a7cf822f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what are the models evaluation metrics ?\"\n",
    "\n",
    "result = await agent.run(user_prompt=question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f7abda2b-dcde-4262-9992-a52c9e066177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModelRequest(parts=[UserPromptPart(content='what are the models evaluation metrics ?', timestamp=datetime.datetime(2025, 9, 28, 21, 27, 12, 562842, tzinfo=datetime.timezone.utc))], instructions='o'),\n",
       " ModelResponse(parts=[ThinkingPart(content='', signature='CsICAdHtim8/9NFe7Hj+6f2S049zi0uF83JasyTy9kEcS9eW1Z3VEpqa296HUPFIo+15XqSn1SYKixzzRjF7PbocI8oiVAyqLyo3HIVGMHUqisc+2Dc2b4ls1lCIr61M5WwLL0NuwkqKXH7eNlc5ZsXSO7m1ei7+f+FOuG6nnQNcDGTTwi98NxlYTK5MPsoAhvSQyuKagxHu1uDA5lNv3k5Bpsok6FXG18CaYjDQBjgjjv4kezbAhH2wmWBLBYgkt9EkvsOQiana6uR4MbXhdyzXHnVTR7iL9ZGCXX6vgFqlR0DjnR+xmFKMfh12EWoGMFjbxiPO6wOX+JMtcDbaiEHvEJbFzHwkyP2DFVemkTfZZRIBp08sHjGT7Uf15zHawURiYLREOJvqx+oTeUgka0f3WssY+6CLkWdjX2xtF2PwMQUvFQ==', provider_name='google-gla'), ToolCallPart(tool_name='text_search', args={'query': 'models evaluation metrics'}, tool_call_id='pyd_ai_79ec27e248844494ae0f4332bb176160')], usage=RequestUsage(input_tokens=105, output_tokens=81, details={'thoughts_tokens': 64, 'text_prompt_tokens': 105}), model_name='gemini-2.5-flash', timestamp=datetime.datetime(2025, 9, 28, 21, 27, 14, 492000, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='L6jZaLuDO8KFvdIP86KjaQ', finish_reason='stop'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='text_search', content=[{'start': 0, 'chunk': '# Insights on Model Evaluation and Fine-Tuning\\n\\n## Introduction\\n\\nOnce you\\'ve [trained](./model-training-tips.md) your computer vision model, evaluating and refining it to perform optimally is essential. Just training your model isn\\'t enough. You need to make sure that your model is accurate, efficient, and fulfills the [objective](./defining-project-goals.md) of your computer vision project. By evaluating and fine-tuning your model, you can identify weaknesses, improve its accuracy, and boost overall performance.\\n\\n<p align=\"center\">\\n  <br>\\n  <iframe loading=\"lazy\" width=\"720\" height=\"405\" src=\"https://www.youtube.com/embed/-aYO-6VaDrw\"\\n    title=\"YouTube video player\" frameborder=\"0\"\\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\\n    allowfullscreen>\\n  </iframe>\\n  <br>\\n  <strong>Watch:</strong> Insights into Model Evaluation and Fine-Tuning | Tips for Improving Mean Average Precision\\n</p>\\n\\nIn this guide, we\\'ll share insights on model evaluation and fine-tuning that\\'ll make this [step of a computer vision project](./steps-of-a-cv-project.md) more approachable. We\\'ll discuss how to understand evaluation metrics and implement fine-tuning techniques, giving you the knowledge to elevate your model\\'s capabilities.\\n\\n## Evaluating Model Performance Using Metrics\\n\\nEvaluating how well a model performs helps us understand how effectively it works. Various metrics are used to measure performance. These [performance metrics](./yolo-performance-metrics.md) provide clear, numerical insights that can guide improvements toward making sure the model meets its intended goals. Let\\'s take a closer look at a few key metrics.\\n\\n### Confidence Score\\n\\nThe confidence score represents the model\\'s certainty that a detected object belongs to a particular class. It ranges from 0 to 1, with higher scores indicating greater confidence. The confidence score helps filter predictions; only detections with confidence scores above a specif', 'comments': True, 'description': 'Explore the most effective ways to assess and refine YOLO11 models for better performance. Learn about evaluation metrics, fine-tuning processes, and how to customize your model for specific needs.', 'keywords': 'Model Evaluation, Machine Learning Model Evaluation, Fine Tuning Machine Learning, Fine Tune Model, Evaluating Models, Model Fine Tuning, How to Fine Tune a Model', 'filename': 'ultralytics-main/docs/en/guides/model-evaluation-insights.md'}, {'start': 13000, 'chunk': ' your dataset and GPU memory. For more details, visit our [section on handling variable image sizes](#handling-variable-image-sizes).\\n\\n### What practical steps can I take to improve mean average precision for my YOLO11 model?\\n\\nImproving mean average precision (mAP) for a YOLO11 model involves several steps:\\n\\n1. **Tuning Hyperparameters**: Experiment with different learning rates, [batch sizes](https://www.ultralytics.com/glossary/batch-size), and image augmentations.\\n2. **[Data Augmentation](https://www.ultralytics.com/glossary/data-augmentation)**: Use techniques like Mosaic and MixUp to create diverse training samples.\\n3. **Image Tiling**: Split larger images into smaller tiles to improve detection accuracy for small objects.\\n   Refer to our detailed guide on [model fine-tuning](#tips-for-fine-tuning-your-model) for specific strategies.\\n\\n### How do I access YOLO11 model evaluation metrics in Python?\\n\\nYou can access YOLO11 model evaluation metrics using Python with the following steps:\\n\\n!!! example \"Usage\"\\n\\n    === \"Python\"\\n\\n        ```python\\n        from ultralytics import YOLO\\n\\n        # Load the model\\n        model = YOLO(\"yolo11n.pt\")\\n\\n        # Run the evaluation\\n        results = model.val(data=\"coco8.yaml\")\\n\\n        # Print specific metrics\\n        print(\"Class indices with average precision:\", results.ap_class_index)\\n        print(\"Average precision for all classes:\", results.box.all_ap)\\n        print(\"Mean average precision at IoU=0.50:\", results.box.map50)\\n        print(\"Mean recall:\", results.box.mr)\\n        ```\\n\\nAnalyzing these metrics helps fine-tune and optimize your YOLO11 model. For a deeper dive, check out our guide on [YOLO11 metrics](../modes/val.md).', 'comments': True, 'description': 'Explore the most effective ways to assess and refine YOLO11 models for better performance. Learn about evaluation metrics, fine-tuning processes, and how to customize your model for specific needs.', 'keywords': 'Model Evaluation, Machine Learning Model Evaluation, Fine Tuning Machine Learning, Fine Tune Model, Evaluating Models, Model Fine Tuning, How to Fine Tune a Model', 'filename': 'ultralytics-main/docs/en/guides/model-evaluation-insights.md'}, {'start': 12000, 'chunk': \" more about these metrics in our [YOLO11 performance metrics guide](./yolo-performance-metrics.md).\\n\\n### How can I fine-tune a pre-trained YOLO11 model for my specific dataset?\\n\\nFine-tuning a pre-trained YOLO11 model involves adjusting its parameters to improve performance on a specific task or dataset. Start by evaluating your model using metrics, then set a higher initial learning rate by adjusting the `warmup_epochs` parameter to 0 for immediate stability. Use parameters like `rect=true` for handling varied image sizes effectively. For more detailed guidance, refer to our section on [fine-tuning YOLO11 models](#how-does-fine-tuning-work).\\n\\n### How can I handle variable image sizes when evaluating my YOLO11 model?\\n\\nTo handle variable image sizes during evaluation, use the `rect=true` parameter in YOLO11, which adjusts the network's stride for each batch based on image sizes. The `imgsz` parameter sets the maximum dimension for image resizing, defaulting to 640. Adjust `imgsz` to suit your dataset and GPU memory. For more details, visit our [section on handling variable image sizes](#handling-variable-image-sizes).\\n\\n### What practical steps can I take to improve mean average precision for my YOLO11 model?\\n\\nImproving mean average precision (mAP) for a YOLO11 model involves several steps:\\n\\n1. **Tuning Hyperparameters**: Experiment with different learning rates, [batch sizes](https://www.ultralytics.com/glossary/batch-size), and image augmentations.\\n2. **[Data Augmentation](https://www.ultralytics.com/glossary/data-augmentation)**: Use techniques like Mosaic and MixUp to create diverse training samples.\\n3. **Image Tiling**: Split larger images into smaller tiles to improve detection accuracy for small objects.\\n   Refer to our detailed guide on [model fine-tuning](#tips-for-fine-tuning-your-model) for specific strategies.\\n\\n### How do I access YOLO11 model evaluation metrics in Python?\\n\\nYou can access YOLO11 model evaluation metrics using Python with the following steps\", 'comments': True, 'description': 'Explore the most effective ways to assess and refine YOLO11 models for better performance. Learn about evaluation metrics, fine-tuning processes, and how to customize your model for specific needs.', 'keywords': 'Model Evaluation, Machine Learning Model Evaluation, Fine Tuning Machine Learning, Fine Tune Model, Evaluating Models, Model Fine Tuning, How to Fine Tune a Model', 'filename': 'ultralytics-main/docs/en/guides/model-evaluation-insights.md'}, {'start': 0, 'chunk': '# Reference for `ultralytics/utils/metrics.py`\\n\\n!!! note\\n\\n    This file is available at [https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/metrics.py](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/utils/metrics.py). If you spot a problem please help fix it by [contributing](https://docs.ultralytics.com/help/contributing/) a [Pull Request](https://github.com/ultralytics/ultralytics/edit/main/ultralytics/utils/metrics.py) üõ†Ô∏è. Thank you üôè!\\n\\n<br>\\n\\n## ::: ultralytics.utils.metrics.ConfusionMatrix\\n\\n<br><br><hr><br>\\n\\n## ::: ultralytics.utils.metrics.Metric\\n\\n<br><br><hr><br>\\n\\n## ::: ultralytics.utils.metrics.DetMetrics\\n\\n<br><br><hr><br>\\n\\n## ::: ultralytics.utils.metrics.SegmentMetrics\\n\\n<br><br><hr><br>\\n\\n## ::: ultralytics.utils.metrics.PoseMetrics\\n\\n<br><br><hr><br>\\n\\n## ::: ultralytics.utils.metrics.ClassifyMetrics\\n\\n<br><br><hr><br>\\n\\n## ::: ultralytics.utils.metrics.OBBMetrics\\n\\n<br><br><hr><br>\\n\\n## ::: ultralytics.utils.metrics.bbox_ioa\\n\\n<br><br><hr><br>\\n\\n## ::: ultralytics.utils.metrics.box_iou\\n\\n<br><br><hr><br>\\n\\n## ::: ultralytics.utils.metrics.bbox_iou\\n\\n<br><br><hr><br>\\n\\n## ::: ultralytics.utils.metrics.mask_iou\\n\\n<br><br><hr><br>\\n\\n## ::: ultralytics.utils.metrics.kpt_iou\\n\\n<br><br><hr><br>\\n\\n## ::: ultralytics.utils.metrics._get_covariance_matrix\\n\\n<br><br><hr><br>\\n\\n## ::: ultralytics.utils.metrics.probiou\\n\\n<br><br><hr><br>\\n\\n## ::: ultralytics.utils.metrics.batch_probiou\\n\\n<br><br><hr><br>\\n\\n## ::: ultralytics.utils.metrics.smooth_bce\\n\\n<br><br><hr><br>\\n\\n## ::: ultralytics.utils.metrics.smooth\\n\\n<br><br><hr><br>\\n\\n## ::: ultralytics.utils.metrics.plot_pr_curve\\n\\n<br><br><hr><br>\\n\\n## ::: ultralytics.utils.metrics.plot_mc_curve\\n\\n<br><br><hr><br>\\n\\n## ::: ultralytics.utils.metrics.compute_ap\\n\\n<br><br><hr><br>\\n\\n## ::: ultralytics.utils.metrics.ap_per_class\\n\\n<br><br>', 'description': \"Explore detailed metrics and utility functions for model validation and performance analysis with Ultralytics' metrics module.\", 'keywords': 'Ultralytics, metrics, model validation, performance analysis, IoU, confusion matrix', 'filename': 'ultralytics-main/docs/en/reference/utils/metrics.md'}, {'start': 1000, 'chunk': \"hts on model evaluation and fine-tuning that'll make this [step of a computer vision project](./steps-of-a-cv-project.md) more approachable. We'll discuss how to understand evaluation metrics and implement fine-tuning techniques, giving you the knowledge to elevate your model's capabilities.\\n\\n## Evaluating Model Performance Using Metrics\\n\\nEvaluating how well a model performs helps us understand how effectively it works. Various metrics are used to measure performance. These [performance metrics](./yolo-performance-metrics.md) provide clear, numerical insights that can guide improvements toward making sure the model meets its intended goals. Let's take a closer look at a few key metrics.\\n\\n### Confidence Score\\n\\nThe confidence score represents the model's certainty that a detected object belongs to a particular class. It ranges from 0 to 1, with higher scores indicating greater confidence. The confidence score helps filter predictions; only detections with confidence scores above a specified threshold are considered valid.\\n\\n_Quick Tip:_ When running inferences, if you aren't seeing any predictions, and you've checked everything else, try lowering the confidence score. Sometimes, the threshold is too high, causing the model to ignore valid predictions. Lowering the score allows the model to consider more possibilities. This might not meet your project goals, but it's a good way to see what the model can do and decide how to fine-tune it.\\n\\n### Intersection over Union\\n\\n[Intersection over Union](https://www.ultralytics.com/glossary/intersection-over-union-iou) (IoU) is a metric in [object detection](https://www.ultralytics.com/glossary/object-detection) that measures how well the predicted [bounding box](https://www.ultralytics.com/glossary/bounding-box) overlaps with the ground truth bounding box. IoU values range from 0 to 1, where one stands for a perfect match. IoU is essential because it measures how closely the predicted boundaries match the actual object boundaries.\", 'comments': True, 'description': 'Explore the most effective ways to assess and refine YOLO11 models for better performance. Learn about evaluation metrics, fine-tuning processes, and how to customize your model for specific needs.', 'keywords': 'Model Evaluation, Machine Learning Model Evaluation, Fine Tuning Machine Learning, Fine Tune Model, Evaluating Models, Model Fine Tuning, How to Fine Tune a Model', 'filename': 'ultralytics-main/docs/en/guides/model-evaluation-insights.md'}], tool_call_id='pyd_ai_79ec27e248844494ae0f4332bb176160', timestamp=datetime.datetime(2025, 9, 28, 21, 27, 14, 538877, tzinfo=datetime.timezone.utc))], instructions='o'),\n",
       " ModelResponse(parts=[ThinkingPart(content='', signature='Cs0IAdHtim9ikTOQZ2ACKscfxY9XWBze8CYZPn58tnSdijSLcmJnDefHaaoLnuWopEvD8ndaackUFIjYoiWkUjSco+gX4dxxQtZHSXtWZ68abaNWoQQsfAtiNCbSd98thsw/FYuZrJZnUlY+PIgNKyyG/qj+0wxxcrsQkor24M8HVN81T6diENsPZhUnfSBCLW5IAUKv3orh5nBIMD3hXfpiBYZcLKchuzr6DDqa2g2nLerQw4rN8XlHvLs1/JQgV+AZ0ApD3nsCuFIdRz4w3GlgjZJKAVh3cYzIjhqHQPwlUAZCYXA+N1EZGsYuYUoklLxIU6zIIuq+Fgh5KO4uY1mXCzxCSbgDtpKL4O8+7ZKe6mySpeHjkqe0N+VAYfvLpsVV7W1iNRdZBZwMtbzd+EhpDtpuQANTkg8pcmHhXUwcqAaMmYX02TxSnIW/EYbS3rBxCquH3Ij92a0KM90YFWIC2RzJ0Lt4vXGevCmjsj90Pd0/tQSEbthyD6I3Mu+DwL7BUKyY7kB0yHXBzkPMRAcbk+f9DlCtfXP+Eue9F7d677DbUiMVOPb/pUJtyLpmOx997MqlsN2S/RqYVU3efVCAY/Ribu/EhWa+boUotIC/mXSIkQaMipHSmRAuR0YKx4jXurq7WwsL37cvqj+YURuRJkeLjfYaylD8knr6wgu0T6WTMBacjG30/rIAOw0Vwxy8M2gZYgHn4D2+xnWXs6I/H4WOjQiev8PB1iKXzl4HywB0ucFRJ8j6Sw7I+CvvBNyqQTviSHPXvfUkwtltuMumGb9VVaRaTH9CyrLd72543Ny5NdLIpUb0S1XfcipqOaShAcnJu5Pyfa6LOJNoxLnqPYcgae46ps+XBMnf2+OuoCGo5xT5UxzzWnmKDVPAv0T60tXASDl2OTKGI/4pwFCFUiWS0O04o/7WfSR4txv5LtJe2OLqXYO8Wu6fJJMS546Yk7Ie4SJkq8w/3F9WzPXNpp7OifjclljQjkPKXSLT43t3Uz/6EDlZm/T8it09KTKZxOH30OxMQcjtF5OhYqRvcIle/uWy1yYFTZyNXh2RSwITRgWsJscVbIx+YjlxaQuDQrYl1wnEx+4LtjqQbpKR9bIX5T10ww58K9QeU5amsSFdRuzsAcdJZWyHJRywIAKyKuAZpFSK5oQUHUvM6vD6TfuKlzJCQZQ1h1+xXtyDuP56JWZzcAz8Xa9ArWMKBQSw7OC+RjKc/5OyrQJiHMHX/5qMKqUlEKjzCkCxsB/twhAu5eAOdxwZdcVpvM5aKSnT43+76rFsihKt8wXLpXCI/XNIVn5E+Iu8sYh2qZEYUQR+9cseWqy510CU6NGDW58wT4+EGpQSsxa+GOV3DUwp2Edoyu80fp0s6sRnDH0dlXIa5Fle2V4O+1osximTCd5KfiSQckrFWNQlnJ+ReV5xR8M5TUkVqsYNLupzT6Ksq0D9HOAmCNLKir1F1JTS', provider_name='google-gla'), TextPart(content=\"To evaluate model performance, various metrics are used to understand how effectively a model works. Key metrics include:\\n\\n*   **Confidence Score**: This represents the model's certainty that a detected object belongs to a particular class, ranging from 0 to 1.\\n*   **Intersection over Union (IoU)**: In object detection, IoU measures the overlap between the predicted bounding box and the ground truth bounding box, with values from 0 to 1, where 1 indicates a perfect match.\\n*   **Mean Average Precision (mAP)**: This is a common metric for object detection models, and improving it often involves tuning hyperparameters, data augmentation, and image tiling.\\n*   **Recall**: Another important metric often considered alongside precision.\\n\\nFor YOLO11 models, specific metrics you can access in Python include:\\n\\n*   `results.ap_class_index`: Class indices with average precision.\\n*   `results.box.all_ap`: Average precision for all classes.\\n*   `results.box.map50`: Mean average precision at IoU=0.50.\\n*   `results.box.mr`: Mean recall.\\n\\nAdditionally, the `ultralytics.utils.metrics` module offers a range of metrics and utility functions for model validation and performance analysis, such as `ConfusionMatrix`, `DetMetrics`, `SegmentMetrics`, `PoseMetrics`, `ClassifyMetrics`, and various IoU-related functions like `bbox_ioa`, `box_iou`, and `mask_iou`.\")], usage=RequestUsage(input_tokens=3301, output_tokens=571, details={'thoughts_tokens': 251, 'text_prompt_tokens': 3301}), model_name='gemini-2.5-flash', timestamp=datetime.datetime(2025, 9, 28, 21, 27, 18, 180807, tzinfo=datetime.timezone.utc), provider_name='google-gla', provider_details={'finish_reason': 'STOP'}, provider_response_id='M6jZaPq5NIDBvdIPgdONEQ', finish_reason='stop')]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.new_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f74cd93d-a2a6-4d25-8bb1-9bf5e8be96e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentRunResult(output=\"To evaluate model performance, various metrics are used to understand how effectively a model works. Key metrics include:\\n\\n*   **Confidence Score**: This represents the model's certainty that a detected object belongs to a particular class, ranging from 0 to 1.\\n*   **Intersection over Union (IoU)**: In object detection, IoU measures the overlap between the predicted bounding box and the ground truth bounding box, with values from 0 to 1, where 1 indicates a perfect match.\\n*   **Mean Average Precision (mAP)**: This is a common metric for object detection models, and improving it often involves tuning hyperparameters, data augmentation, and image tiling.\\n*   **Recall**: Another important metric often considered alongside precision.\\n\\nFor YOLO11 models, specific metrics you can access in Python include:\\n\\n*   `results.ap_class_index`: Class indices with average precision.\\n*   `results.box.all_ap`: Average precision for all classes.\\n*   `results.box.map50`: Mean average precision at IoU=0.50.\\n*   `results.box.mr`: Mean recall.\\n\\nAdditionally, the `ultralytics.utils.metrics` module offers a range of metrics and utility functions for model validation and performance analysis, such as `ConfusionMatrix`, `DetMetrics`, `SegmentMetrics`, `PoseMetrics`, `ClassifyMetrics`, and various IoU-related functions like `bbox_ioa`, `box_iou`, and `mask_iou`.\")"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cd21df-3b50-49e7-8ae2-d9f8d3801782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
